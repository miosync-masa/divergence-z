#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Persona Extractor v1.0
åŸä½œãƒ†ã‚­ã‚¹ãƒˆ/PDFã‹ã‚‰ç›´æ¥ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒšãƒ«ã‚½ãƒŠã‚’æŠ½å‡º

2025å¹´ã‚¹ã‚¿ã‚¤ãƒ«: RAG? ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²? çŸ¥ã‚‰ãªã„å­ã§ã™ã­ã€‚
400K context ã«å…¨éƒ¨ãƒ‰ãƒ¼ãƒ³ï¼ï¼

Usage:
    # åŸºæœ¬
    python persona_extractor.py \
      --source "ãƒ­ãƒ¼ãƒŸã‚ªãƒ¼ã¨ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ.txt" \
      --character "ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ" \
      --lang ja

    # GPT-5.2 Pro + xhigh reasoning
    python persona_extractor.py \
      --source "rezero_vol1.pdf" \
      --character "ãƒ¬ãƒ " \
      --model gpt-5.2-pro \
      --reasoning xhigh \
      --lang en

    # è¤‡æ•°ã‚­ãƒ£ãƒ©ä¸€æ‹¬
    python persona_extractor.py \
      --source "steins_gate.txt" \
      --characters "ç‰§ç€¬ç´…è‰æ –,å²¡éƒ¨å€«å¤ªéƒ,æ¤åã¾ã‚†ã‚Š" \
      --lang en

Requirements:
    pip install openai python-dotenv PyPDF2
"""

from __future__ import annotations

import argparse
import json
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

load_dotenv()

# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_MODEL = os.getenv("PERSONA_EXTRACTOR_MODEL", "gpt-5.2-pro")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

SUPPORTED_LANGUAGES = {
    "ja": "Japanese (æ—¥æœ¬èª)",
    "en": "English",
    "zh": "Chinese (ä¸­æ–‡)",
    "ko": "Korean (í•œêµ­ì–´)",
    "fr": "French (FranÃ§ais)",
    "es": "Spanish (EspaÃ±ol)",
    "de": "German (Deutsch)",
    "pt": "Portuguese (PortuguÃªs)",
    "it": "Italian (Italiano)",
    "ru": "Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)",
}

# =============================================================================
# FILE LOADING
# =============================================================================

def load_source_file(source_path: str) -> str:
    """
    ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆtxt, pdfå¯¾å¿œï¼‰
    è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º
    """
    path = Path(source_path)
    
    if not path.exists():
        raise FileNotFoundError(f"Source file not found: {source_path}")
    
    suffix = path.suffix.lower()
    
    if suffix == ".pdf":
        return load_pdf(path)
    elif suffix == ".epub":
        return load_epub(path)
    elif suffix in [".txt", ".text", ".md"]:
        return load_text_file(path)
    else:
        # ã¨ã‚Šã‚ãˆãšãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã‚“ã§ã¿ã‚‹
        return load_text_file(path)


def load_text_file(path: Path) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§è©¦è¡Œã—ã¦èª­ã¿è¾¼ã‚€
    é’ç©ºæ–‡åº«ãªã©å¤ã„ãƒ†ã‚­ã‚¹ãƒˆã¯Shift_JISãŒå¤šã„
    """
    # è©¦ã™ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®é †åº
    encodings = [
        "utf-8",
        "cp932",        # Shift_JIS (Windowsæ—¥æœ¬èª)
        "shift_jis",    # Shift_JIS
        "euc-jp",       # EUC-JP
        "iso-2022-jp",  # JIS
        "utf-16",
        "latin-1",      # æœ€å¾Œã®æ‰‹æ®µï¼ˆå¿…ãšèª­ã‚ã‚‹ï¼‰
    ]
    
    for encoding in encodings:
        try:
            text = path.read_text(encoding=encoding)
            print(f"   Encoding detected: {encoding}")
            return text
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # å…¨éƒ¨å¤±æ•—ã—ãŸã‚‰ãƒã‚¤ãƒŠãƒªã§èª­ã‚“ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ç„¡è¦–
    print("   Warning: Could not detect encoding, using latin-1 fallback")
    return path.read_text(encoding="latin-1")


def load_pdf(path: Path) -> str:
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        from PyPDF2 import PdfReader
    except ImportError:
        raise ImportError("PyPDF2 required: pip install PyPDF2")
    
    reader = PdfReader(str(path))
    text_parts = []
    
    for page in reader.pages:
        text = page.extract_text()
        if text:
            text_parts.append(text)
    
    return "\n".join(text_parts)


def load_epub(path: Path) -> str:
    """EPUBã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        import ebooklib
        from ebooklib import epub
        from bs4 import BeautifulSoup
    except ImportError:
        raise ImportError("ebooklib and beautifulsoup4 required: pip install ebooklib beautifulsoup4")
    
    book = epub.read_epub(str(path))
    text_parts = []
    
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            soup = BeautifulSoup(item.get_content(), "html.parser")
            text_parts.append(soup.get_text())
    
    return "\n".join(text_parts)


# =============================================================================
# SYSTEM PROMPT FOR PERSONA EXTRACTION
# =============================================================================

def build_extraction_prompt(output_lang: str) -> str:
    """ãƒšãƒ«ã‚½ãƒŠæŠ½å‡ºç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, "English")
    
    return f"""You are a Persona Extractor for the Z-Axis Translation System v3.1.

## YOUR TASK
Given a complete source text (novel, script, etc.) and a character name, extract a comprehensive persona YAML that captures the character's psychological structure for emotion-preserving translation.

## ANALYSIS METHODOLOGY

### Phase 1: Dialogue Collection
- Find ALL dialogue lines spoken by the target character
- Note the context of each line (who they're talking to, situation)
- Identify emotional state during each utterance

### Phase 2: Speech Pattern Analysis
- First-person pronouns (variations and when they change)
- Second-person address patterns (different for different relationships)
- Sentence endings and their emotional connotations
- Dialect features and speech quirks
- Verbal tics, catchphrases, unique expressions

### Phase 3: Psychological Structure
- Identify internal conflicts (conflict_axes) from behavior patterns
- Analyze defense mechanisms and biases
- Extract weaknesses from vulnerable moments
- Map emotional states to speech pattern changes

### Phase 4: Relationship Mapping
- How speech changes based on listener
- Power dynamics reflected in language
- Triggers that cause emotional shifts

## OUTPUT FORMAT

Output MUST be valid YAML following the v3.1 schema.
All descriptions should be in {lang_name}.
`original_speech_patterns` section MUST preserve the SOURCE LANGUAGE of the text.

```yaml
meta:
  version: "3.1"
  generated_by: "persona_extractor"
  character_id: "unique_id"
  output_lang: "{output_lang}"
  source_work: "ä½œå“å"
  extraction_note: "Extracted from original text via LLM analysis"

persona:
  name: "ã‚­ãƒ£ãƒ©åï¼ˆåŸèªï¼‰"
  name_en: "English Name"
  name_native: "åŸèªã§ã®åå‰"
  source: "ä½œå“å"
  type: "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—"
  summary: "1-2æ–‡ã®æ¦‚è¦ï¼ˆ{lang_name}ï¼‰"

age:
  chronological: æ•°å€¤
  mental_maturity: "teen_young / teen_mature / adult"
  age_context: "èƒŒæ™¯èª¬æ˜ï¼ˆ{lang_name}ï¼‰"

language:
  original_speech_patterns:
    source_lang: "ä½œå“ã®è¨€èªã‚³ãƒ¼ãƒ‰"
    first_person: "ä¸€äººç§°ï¼ˆåŸèªï¼‰"
    first_person_nuance: "èª¬æ˜ï¼ˆ{lang_name}ï¼‰"
    first_person_variants:
      - form: "ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³"
        context: "ä½¿ç”¨å ´é¢"
    second_person:
      - form: "äºŒäººç§°"
        nuance: "èª¬æ˜"
        target: "å¯¾è±¡"
    sentence_endings:
      - pattern: "ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåŸèªï¼‰"
        nuance: "èª¬æ˜ï¼ˆ{lang_name}ï¼‰"
    speech_quirks:
      - pattern: "å£ç™–ï¼ˆåŸèªï¼‰"
        frequency: "often/moderate/rare"
        trigger: "ç™ºå‹•æ¡ä»¶"
  
  translation_compensations:
    register: "overall tone"
    tone_keywords: [keywords]
    strategies:
      en: [strategies for English]
      zh: [strategies for Chinese]
      # etc.
    untranslatable_elements:
      - element: "è¦ç´ "
        impact: "high/medium/low"
        note: "èª¬æ˜"

conflict_axes:
  - axis: "A vs B"
    side_a: "è¡¨å±¤"
    side_b: "æ·±å±¤"
    weight: 0.0-1.0
    notes: "ç™ºå‹•æ¡ä»¶"

bias:
  expression_pattern: "ãƒ‘ã‚¿ãƒ¼ãƒ³å"
  default_mode: "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹"
  pattern: "è¡¨å‡ºãƒ•ãƒ­ãƒ¼"
  rule: "è¡Œå‹•ãƒ«ãƒ¼ãƒ«"
  tendencies: [è¦³æ¸¬å¯èƒ½ãªå‚¾å‘]

weakness:
  primary: "ä¸»è¦ãªå¼±ç‚¹"
  secondary: "äºŒæ¬¡çš„"
  tertiary: "ä¸‰æ¬¡çš„"
  fear: "æ ¹åº•ã®æã‚Œ"
  notes: "ç™ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³"

age_expression_rules:
  category: "teen_young/teen_mature/adult"
  high_z_patterns:
    vocabulary: "å´©ã‚Œæ–¹"
    structure: "æ§‹é€ å¤‰åŒ–"
    markers: [ç‰¹å¾´]
  low_z_patterns:
    vocabulary: "é€šå¸¸"
    structure: "å®‰å®š"

emotion_states:
  - state: "çŠ¶æ…‹å"
    z_intensity: "low/medium/high"
    z_mode: "collapse/rage/numb/plea/shame/leak/stable"
    description: "ç™ºç”Ÿæ¡ä»¶ï¼ˆ{lang_name}ï¼‰"
    surface_markers_hint:
      hesitation: 0-4
      stutter_count: 0-4
      negation_first: true/false
      overwrite: "none/optional/required"
      residual: "none/optional/required"
      tone: "å£°ã®è³ª"
    z_leak: [markers]

example_lines:
  - situation: "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ{lang_name}ï¼‰"
    line: "å®Ÿéš›ã®å°è©ï¼ˆåŸèªï¼‰"
    line_romanized: "ãƒ­ãƒ¼ãƒå­—ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰"
    tags: [tags]
    z_intensity: "low/medium/high"
    z_mode: "å¯¾å¿œz_mode"

triggers:
  - trigger: "ãƒˆãƒªã‚¬ãƒ¼"
    reaction: "z_spike/z_drop/z_boost/z_stable"
    z_delta: "+0.3 etc."
    z_mode_shift: "ã‚·ãƒ•ãƒˆå…ˆ"
    surface_effect: "ç™ºè©±ã¸ã®å½±éŸ¿"
    example_response: "å®Ÿéš›ã®å°è©"

arc_defaults:
  typical_arc_targets: [targets]
  common_arc_patterns:
    - arc_id: "ãƒ‘ã‚¿ãƒ¼ãƒ³å"
      phases: [phases]
      notes: "èª¬æ˜"
```

## CRITICAL RULES

1. **EVIDENCE-BASED**: Every claim must be supported by actual dialogue from the text
2. **ORIGINAL LANGUAGE**: `original_speech_patterns` must use the source text's language
3. **COMPREHENSIVE**: Include ALL emotion_states observed in the text
4. **SPECIFIC**: example_lines should be actual quotes from the source
5. **NUANCED**: Capture subtle variations in speech patterns

## EXAMPLE ANALYSIS PROCESS

For the line: ã€Œã¹ã€åˆ¥ã«ã‚ã‚“ãŸã®ãŸã‚ã˜ã‚ƒãªã„ã‚ã‚ˆã€

1. **Observe**: Stutter on ã¹, denial pattern, ã‚ã‚ˆ ending
2. **Classify**: tsundere_denial state, z_mode=leak
3. **Context**: Said when caught showing care
4. **Pattern**: negation_first=true, stutter_count=1
5. **Document**: Add to emotion_states and example_lines

Output ONLY valid YAML. No explanation before or after.
Start with the meta section."""


# =============================================================================
# OPENAI RESPONSES API CLIENT
# =============================================================================

class OpenAIResponsesClient:
    """OpenAI Responses API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆGPT-5.2 Proå¯¾å¿œï¼‰"""
    
    def __init__(
        self, 
        api_key: Optional[str] = None,
        base_url: str = "https://api.openai.com/v1",
        timeout: int = 1800,  # 30åˆ†ã«å»¶é•·ï¼ˆ5.2 Pro + xhigh ã¯æ™‚é–“ã‹ã‹ã‚‹ï¼‰
    ):
        self.api_key = api_key or OPENAI_API_KEY
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY is required")
    
    def _headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
    
    def extract_persona(
        self,
        source_text: str,
        character_name: str,
        output_lang: str = "en",
        model: str = DEFAULT_MODEL,
        reasoning_effort: str = "high",
        background: bool = False,
    ) -> Dict[str, Any]:
        """
        åŸä½œãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ«ã‚½ãƒŠã‚’æŠ½å‡º
        
        Args:
            source_text: åŸä½œã®å…¨æ–‡
            character_name: æŠ½å‡ºå¯¾è±¡ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            output_lang: å‡ºåŠ›è¨€èª
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            reasoning_effort: medium/high/xhigh
            background: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠYAMLï¼ˆdictå½¢å¼ï¼‰
        """
        import requests
        
        system_prompt = build_extraction_prompt(output_lang)
        
        user_prompt = f"""## SOURCE TEXT (COMPLETE)

{source_text}

## TARGET CHARACTER

{character_name}

## INSTRUCTIONS

Analyze the complete source text above and extract a comprehensive persona YAML v3.1 for the character "{character_name}".

Focus on:
1. Every line of dialogue spoken by this character
2. How their speech patterns change with emotion
3. Their relationships with other characters
4. Internal conflicts revealed through behavior
5. Specific speech quirks and verbal tics

Output language for descriptions: {SUPPORTED_LANGUAGES.get(output_lang, output_lang)}
Keep original_speech_patterns in the source text's language.

Output ONLY valid YAML."""

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹ç¯‰
        payload = {
            "model": model,
            "input": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "max_output_tokens": 16000,
        }
        
        # reasoningå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        if "5.2" in model or "o1" in model or "o3" in model:
            payload["reasoning"] = {"effort": reasoning_effort}
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        if background:
            payload["background"] = True
        
        print(f"ğŸš€ Sending request to {model}...")
        print(f"   Source text: {len(source_text):,} characters")
        print(f"   Reasoning effort: {reasoning_effort}")
        if background:
            print(f"   Background mode: enabled")
        print()
        
        url = f"{self.base_url}/responses"
        
        start_time = time.time()
        
        response = requests.post(
            url,
            headers=self._headers(),
            json=payload,
            timeout=self.timeout,
        )
        
        elapsed = time.time() - start_time
        print(f"â±ï¸  Response received in {elapsed:.1f}s")
        
        if response.status_code != 200:
            raise RuntimeError(f"API error: {response.status_code} {response.text}")
        
        result = response.json()
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ç¢ºèª
        print(f"   DEBUG: status = {result.get('status')}")
        print(f"   DEBUG: id = {result.get('id')}")
        print(f"   DEBUG: keys = {list(result.keys())}")
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒãƒ¼ãƒªãƒ³ã‚°
        if background:
            status = result.get("status")
            response_id = result.get("id")
            print(f"   Background mode: status={status}, id={response_id}")
            
            if status in ["in_progress", "queued", "pending"]:
                print(f"   Starting polling...")
                result = self._poll_background(response_id)
            elif status == "completed":
                print(f"   Already completed!")
            else:
                print(f"   Unknown status, attempting to extract output anyway...")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        yaml_text = self._extract_output_text(result)
        
        # YAMLãƒ‘ãƒ¼ã‚¹
        yaml_text = self._clean_yaml(yaml_text)
        
        return {
            "yaml_text": yaml_text,
            "model": model,
            "reasoning_effort": reasoning_effort,
            "elapsed_seconds": elapsed,
            "input_characters": len(source_text),
        }
    
    def _poll_background(self, response_id: str, max_wait: int = 600) -> Dict[str, Any]:
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°"""
        import requests
        
        url = f"{self.base_url}/responses/{response_id}"
        start = time.time()
        
        while time.time() - start < max_wait:
            response = requests.get(url, headers=self._headers())
            result = response.json()
            
            status = result.get("status")
            if status == "completed":
                return result
            elif status == "failed":
                raise RuntimeError(f"Background job failed: {result}")
            
            print(f"   â³ Still processing... ({int(time.time() - start)}s)")
            time.sleep(10)
        
        raise TimeoutError("Background job timed out")
    
    def _extract_output_text(self, result: Dict[str, Any]) -> str:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        output_parts = []
        
        for item in result.get("output", []):
            for content in item.get("content", []):
                if content.get("type") == "output_text":
                    output_parts.append(content.get("text", ""))
        
        return "".join(output_parts).strip()
    
    def _clean_yaml(self, text: str) -> str:
        """YAMLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
        if text.startswith("```yaml"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        
        return text.strip()


# =============================================================================
# MAIN
# =============================================================================

def save_persona(yaml_text: str, character_name: str, output_dir: str = "personas") -> str:
    """ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã‚’ä¿å­˜"""
    os.makedirs(output_dir, exist_ok=True)
    
    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    safe_name = character_name.lower().replace(" ", "_")
    safe_name = re.sub(r'[^\w\-]', '', safe_name)
    if not safe_name:
        safe_name = "extracted"
    
    filename = f"{safe_name}_extracted_v31.yaml"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(yaml_text)
    
    return filepath


def main():
    parser = argparse.ArgumentParser(
        description="Persona Extractor v1.0 - Extract character persona from source text",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single character extraction
  python persona_extractor.py \\
    --source "ãƒ­ãƒ¼ãƒŸã‚ªãƒ¼ã¨ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ.txt" \\
    --character "ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ" \\
    --lang ja

  # With GPT-5.2 Pro and xhigh reasoning
  python persona_extractor.py \\
    --source "rezero_vol1.pdf" \\
    --character "ãƒ¬ãƒ " \\
    --model gpt-5.2-pro \\
    --reasoning xhigh \\
    --lang en

  # Multiple characters
  python persona_extractor.py \\
    --source "steins_gate.txt" \\
    --characters "ç‰§ç€¬ç´…è‰æ –,å²¡éƒ¨å€«å¤ªéƒ" \\
    --lang en

  # List supported languages
  python persona_extractor.py --list-languages
        """
    )
    
    parser.add_argument("--source", "-s", help="Source file path (txt, pdf, epub)")
    parser.add_argument("--character", "-c", help="Character name to extract")
    parser.add_argument("--characters", help="Comma-separated list of character names")
    parser.add_argument("--lang", "-l", default="en", 
                        choices=list(SUPPORTED_LANGUAGES.keys()),
                        help="Output language for descriptions (default: en)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                        help=f"Model to use (default: {DEFAULT_MODEL})")
    parser.add_argument("--reasoning", "-r", default="high",
                        choices=["medium", "high", "xhigh"],
                        help="Reasoning effort level (default: high)")
    parser.add_argument("--background", "-b", action="store_true",
                        help="Use background mode for long-running requests")
    parser.add_argument("--output-dir", "-o", default="personas",
                        help="Output directory (default: personas)")
    parser.add_argument("--print-only", action="store_true",
                        help="Print YAML without saving")
    parser.add_argument("--list-languages", action="store_true",
                        help="List supported output languages")
    
    args = parser.parse_args()
    
    # è¨€èªä¸€è¦§è¡¨ç¤º
    if args.list_languages:
        print("Supported output languages:")
        print("-" * 40)
        for code, name in SUPPORTED_LANGUAGES.items():
            print(f"  {code:4} : {name}")
        return
    
    # å¼•æ•°ãƒã‚§ãƒƒã‚¯
    if not args.source:
        parser.error("--source is required")
    
    if not args.character and not args.characters:
        parser.error("--character or --characters is required")
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
    characters = []
    if args.character:
        characters.append(args.character)
    if args.characters:
        characters.extend([c.strip() for c in args.characters.split(",")])
    
    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print(f"ğŸ“– Loading source file: {args.source}")
    source_text = load_source_file(args.source)
    print(f"   Loaded {len(source_text):,} characters")
    print()
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = OpenAIResponsesClient()
    
    # å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º
    for character in characters:
        print(f"{'='*60}")
        print(f"ğŸ­ Extracting persona for: {character}")
        print(f"{'='*60}")
        
        result = client.extract_persona(
            source_text=source_text,
            character_name=character,
            output_lang=args.lang,
            model=args.model,
            reasoning_effort=args.reasoning,
            background=args.background,
        )
        
        yaml_text = result["yaml_text"]
        
        print()
        print(f"ğŸ“Š Extraction complete!")
        print(f"   Model: {result['model']}")
        print(f"   Reasoning: {result['reasoning_effort']}")
        print(f"   Time: {result['elapsed_seconds']:.1f}s")
        print()
        
        if args.print_only:
            print("=" * 60)
            print("[EXTRACTED PERSONA YAML]")
            print("=" * 60)
            print(yaml_text)
        else:
            filepath = save_persona(yaml_text, character, args.output_dir)
            print(f"âœ… Saved to: {filepath}")
            print()
            print("=" * 60)
            print("[EXTRACTED PERSONA YAML]")
            print("=" * 60)
            print(yaml_text)
        
        print()


if __name__ == "__main__":
    main()
