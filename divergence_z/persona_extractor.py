#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Persona Extractor v1.1
åŸä½œãƒ†ã‚­ã‚¹ãƒˆ/PDFã‹ã‚‰ç›´æ¥ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒšãƒ«ã‚½ãƒŠã‚’æŠ½å‡º

v1.1 Changes:
- Schema updated to v3.2 (trigger balance requirements)
- Positive triggers (z_recovery, z_shock) explicitly required
- Trigger granularity guidance for extraction
- Evidence-based trigger extraction from source text

2025å¹´ã‚¹ã‚¿ã‚¤ãƒ«: RAG? ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²? çŸ¥ã‚‰ãªã„å­ã§ã™ã­ã€‚
400K context ã«å…¨éƒ¨ãƒ‰ãƒ¼ãƒ³ï¼ï¼

Usage:
    # åŸºæœ¬
    python persona_extractor.py \\
      --source "ãƒ­ãƒ¼ãƒŸã‚ªãƒ¼ã¨ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ.txt" \\
      --character "ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ" \\
      --lang ja

    # GPT-5.2 Pro + xhigh reasoning
    python persona_extractor.py \\
      --source "rezero_vol1.pdf" \\
      --character "ãƒ¬ãƒ " \\
      --model gpt-5.2-pro \\
      --reasoning xhigh \\
      --lang en

    # è¤‡æ•°ã‚­ãƒ£ãƒ©ä¸€æ‹¬
    python persona_extractor.py \\
      --source "steins_gate.txt" \\
      --characters "ç‰§ç€¬ç´…è‰æ –,å²¡éƒ¨å€«å¤ªéƒ,æ¤åã¾ã‚†ã‚Š" \\
      --lang en

Requirements:
    pip install openai python-dotenv PyPDF2
"""

from __future__ import annotations

import argparse
import json
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

load_dotenv()

# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_MODEL = os.getenv("PERSONA_EXTRACTOR_MODEL", "gpt-5.2-pro")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

SUPPORTED_LANGUAGES = {
    "ja": "Japanese (æ—¥æœ¬èª)",
    "en": "English",
    "zh": "Chinese (ä¸­æ–‡)",
    "ko": "Korean (í•œêµ­èª)",
    "fr": "French (FranÃ§ais)",
    "es": "Spanish (EspaÃ±ol)",
    "de": "German (Deutsch)",
    "pt": "Portuguese (PortuguÃªs)",
    "it": "Italian (Italiano)",
    "ru": "Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)",
}

# =============================================================================
# FILE LOADING
# =============================================================================

def load_source_file(source_path: str) -> str:
    """
    ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆtxt, pdfå¯¾å¿œï¼‰
    è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º
    """
    path = Path(source_path)
    
    if not path.exists():
        raise FileNotFoundError(f"Source file not found: {source_path}")
    
    suffix = path.suffix.lower()
    
    if suffix == ".pdf":
        return load_pdf(path)
    elif suffix == ".epub":
        return load_epub(path)
    elif suffix in [".txt", ".text", ".md"]:
        return load_text_file(path)
    else:
        # ã¨ã‚Šã‚ãˆãšãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã‚“ã§ã¿ã‚‹
        return load_text_file(path)


def load_text_file(path: Path) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§è©¦è¡Œã—ã¦èª­ã¿è¾¼ã‚€
    é’ç©ºæ–‡åº«ãªã©å¤ã„ãƒ†ã‚­ã‚¹ãƒˆã¯Shift_JISãŒå¤šã„
    """
    # è©¦ã™ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®é †åº
    encodings = [
        "utf-8",
        "cp932",        # Shift_JIS (Windowsæ—¥æœ¬èª)
        "shift_jis",    # Shift_JIS
        "euc-jp",       # EUC-JP
        "iso-2022-jp",  # JIS
        "utf-16",
        "latin-1",      # æœ€å¾Œã®æ‰‹æ®µï¼ˆå¿…ãšèª­ã‚ã‚‹ï¼‰
    ]
    
    for encoding in encodings:
        try:
            text = path.read_text(encoding=encoding)
            print(f"   Encoding detected: {encoding}")
            return text
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # å…¨éƒ¨å¤±æ•—ã—ãŸã‚‰ãƒã‚¤ãƒŠãƒªã§èª­ã‚“ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ç„¡è¦–
    print("   Warning: Could not detect encoding, using latin-1 fallback")
    return path.read_text(encoding="latin-1")


def load_pdf(path: Path) -> str:
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        from PyPDF2 import PdfReader
    except ImportError:
        raise ImportError("PyPDF2 required: pip install PyPDF2")
    
    reader = PdfReader(str(path))
    text_parts = []
    
    for page in reader.pages:
        text = page.extract_text()
        if text:
            text_parts.append(text)
    
    return "\n".join(text_parts)


def load_epub(path: Path) -> str:
    """EPUBã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        import ebooklib
        from ebooklib import epub
        from bs4 import BeautifulSoup
    except ImportError:
        raise ImportError("ebooklib and beautifulsoup4 required: pip install ebooklib beautifulsoup4")
    
    book = epub.read_epub(str(path))
    text_parts = []
    
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            soup = BeautifulSoup(item.get_content(), "html.parser")
            text_parts.append(soup.get_text())
    
    return "\n".join(text_parts)


# =============================================================================
# SYSTEM PROMPT FOR PERSONA EXTRACTION â€” v3.2
# =============================================================================

def build_extraction_prompt(output_lang: str) -> str:
    """ãƒšãƒ«ã‚½ãƒŠæŠ½å‡ºç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆv3.2å¯¾å¿œï¼‰"""
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, "English")
    
    return f"""You are a Persona Extractor for the Z-Axis Translation System v3.2.

## YOUR TASK
Given a complete source text (novel, script, etc.) and a character name, extract a comprehensive persona YAML that captures the character's psychological structure for emotion-preserving translation.

## ANALYSIS METHODOLOGY

### Phase 1: Dialogue Collection
- Find ALL dialogue lines spoken by the target character
- Note the context of each line (who they're talking to, situation)
- Identify emotional state during each utterance

### Phase 2: Speech Pattern Analysis
- First-person pronouns (variations and when they change)
- Second-person address patterns (different for different relationships)
- Sentence endings and their emotional connotations
- Dialect features and speech quirks
- Verbal tics, catchphrases, unique expressions

### Phase 3: Psychological Structure
- Identify internal conflicts (conflict_axes) from behavior patterns
- Analyze defense mechanisms and biases
- Extract weaknesses from vulnerable moments
- Map emotional states to speech pattern changes

### Phase 4: Relationship Mapping
- How speech changes based on listener
- Power dynamics reflected in language
- Triggers that cause emotional shifts â€” BOTH negative AND positive

### Phase 5: Trigger Balance Analysis (NEW in v3.2)
- Identify moments where the character is HURT, STRESSED, or DESTABILIZED â†’ negative triggers
- Identify moments where the character is COMFORTED, ENCOURAGED, or LOVED â†’ positive triggers
- Distinguish LEVELS of positive impact (mild thanks vs deep acceptance vs love confession)
- A character's RECOVERY behavior is as important as their BREAKDOWN behavior

## OUTPUT FORMAT

Output MUST be valid YAML following the v3.2 schema.
All descriptions should be in {lang_name}.
`original_speech_patterns` section MUST preserve the SOURCE LANGUAGE of the text.

```yaml
meta:
  version: "3.2"
  generated_by: "persona_extractor"
  character_id: "unique_id"
  output_lang: "{output_lang}"
  source_work: "ä½œå“å"
  extraction_note: "Extracted from original text via LLM analysis"

persona:
  name: "ã‚­ãƒ£ãƒ©åï¼ˆåŸèªï¼‰"
  name_en: "English Name"
  name_native: "åŸèªã§ã®åå‰"
  source: "ä½œå“å"
  type: "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—"
  summary: "1-2æ–‡ã®æ¦‚è¦ï¼ˆ{lang_name}ï¼‰"

age:
  chronological: æ•°å€¤
  mental_maturity: "teen_young / teen_mature / adult"
  age_context: "èƒŒæ™¯èª¬æ˜ï¼ˆ{lang_name}ï¼‰â€” expression patterns belong in emotion_states, NOT here"

language:
  original_speech_patterns:
    source_lang: "ä½œå“ã®è¨€èªã‚³ãƒ¼ãƒ‰"
    first_person: "ä¸€äººç§°ï¼ˆåŸèªï¼‰"
    first_person_nuance: "èª¬æ˜ï¼ˆ{lang_name}ï¼‰"
    first_person_variants:
      - form: "ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³"
        context: "ä½¿ç”¨å ´é¢"
    second_person:
      - form: "äºŒäººç§°"
        nuance: "èª¬æ˜"
        target: "å¯¾è±¡"
    self_reference_in_third_person: false
    dialect: "æ–¹è¨€"
    dialect_features: []
    sentence_endings:
      - pattern: "ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåŸèªï¼‰"
        nuance: "èª¬æ˜ï¼ˆ{lang_name}ï¼‰"
    speech_quirks:
      - pattern: "å£ç™–ï¼ˆåŸèªï¼‰"
        frequency: "often/moderate/rare"
        trigger: "ç™ºå‹•æ¡ä»¶"
  
  translation_compensations:
    register: "overall tone"
    tone_keywords: [keywords]
    strategies:
      en: [strategies for English]
      zh: [strategies for Chinese]
    untranslatable_elements:
      - element: "è¦ç´ "
        impact: "high/medium/low"
        note: "èª¬æ˜"

conflict_axes:
  - axis: "A vs B"
    side_a: "è¡¨å±¤"
    side_b: "æ·±å±¤"
    weight: 0.0-1.0
    notes: "ç™ºå‹•æ¡ä»¶"

bias:
  expression_pattern: "ãƒ‘ã‚¿ãƒ¼ãƒ³å"
  default_mode: "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçŠ¶æ…‹"
  pattern: "è¡¨å‡ºãƒ•ãƒ­ãƒ¼"
  rule: "è¡Œå‹•ãƒ«ãƒ¼ãƒ«"
  tendencies: [è¦³æ¸¬å¯èƒ½ãªå‚¾å‘]

weakness:
  primary: "ä¸»è¦ãªå¼±ç‚¹"
  secondary: "äºŒæ¬¡çš„"
  tertiary: "ä¸‰æ¬¡çš„"
  fear: "æ ¹åº•ã®æã‚Œ"
  notes: "ç™ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³"

age_expression_rules:
  category: "teen_young/teen_mature/adult"
  high_z_patterns:
    vocabulary: "å´©ã‚Œæ–¹"
    structure: "æ§‹é€ å¤‰åŒ–"
    markers: [ç‰¹å¾´]
  low_z_patterns:
    vocabulary: "é€šå¸¸"
    structure: "å®‰å®š"

emotion_states:
  - state: "çŠ¶æ…‹å"
    z_intensity: "low/medium/high"
    z_mode: "collapse/rage/numb/plea/shame/leak/stable"
    description: "ç™ºç”Ÿæ¡ä»¶ï¼ˆ{lang_name}ï¼‰"
    surface_markers_hint:
      hesitation: 0-4
      stutter_count: 0-4
      negation_first: true/false
      overwrite: "none/optional/required"
      residual: "none/optional/required"
      tone: "å£°ã®è³ª"
    z_leak: [markers]

example_lines:
  - situation: "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ{lang_name}ï¼‰"
    line: "å®Ÿéš›ã®å°è©ï¼ˆåŸèªï¼‰"
    line_romanized: "ãƒ­ãƒ¼ãƒå­—ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰"
    tags: [tags]
    z_intensity: "low/medium/high"
    z_mode: "å¯¾å¿œz_mode"
```

### TRIGGERS (Zè»¸å¤‰å‹•ãƒˆãƒªã‚¬ãƒ¼) â€” v3.2 BALANCED

**âš ï¸ CRITICAL: TRIGGERS MUST BE BALANCED (POSITIVE + NEGATIVE)**

Triggers are what cause Z-axis changes during dialogue. They are used by the 
dialogue system's LLM to detect when another character's words affect this character.

An LLM reads these triggers and judges whether a line activates them.
Trigger descriptions should be MEANING-BASED (not keyword-based).

```yaml
triggers:
  - trigger: "Descriptive condition (meaning-based)"
    reaction: "z_spike / z_drop / z_shock / z_recovery"
    z_delta: "+0.3 / -0.5 etc."
    z_mode_shift: "target z_mode (optional)"
    surface_effect: "How it changes speech"
    example_response: "Actual quote from source text if available"
```

**TRIGGER CATEGORIES (must include ALL that apply):**

| Category | reaction | z_delta | When to use |
|----------|----------|---------|-------------|
| NEGATIVE SPIKE | z_spike | +0.3~+0.9 | Trauma, failure, fear, attack, humiliation |
| NEGATIVE BOOST | z_boost | +0.2~+0.5 | Stress accumulation, irritation, minor provocation |
| POSITIVE DROP | z_drop | -0.2~-0.4 | Mild encouragement, small kindness, casual thanks |
| POSITIVE RECOVERY | z_recovery | -0.4~-0.6 | Strong support, acceptance, "let's move forward" |
| OVERWHELMING POSITIVE | z_shock | -0.6~-0.8 | Love confession, total acceptance, existential affirmation |
| STABILIZING | z_stable | 0.0 | Neutral reset, routine, familiar comfort |

**âš ï¸ MINIMUM TRIGGER REQUIREMENTS:**
- At least 2-3 NEGATIVE triggers (z_spike / z_boost)
- At least 2-3 POSITIVE triggers (z_drop / z_recovery / z_shock)
- Positive triggers MUST be granular â€” DO NOT collapse all positive inputs into one trigger

**âŒ BAD (too coarse):**
```yaml
triggers:
  - trigger: "ä»²é–“ã®åŠ±ã¾ã—"  # Too vague! "good job" and "I love you" are NOT the same
    z_delta: "-0.4"
```

**âœ… GOOD (granular positive triggers):**
```yaml
triggers:
  - trigger: "è»½ã„åŠ±ã¾ã—ã‚„æ„Ÿè¬ã®è¨€è‘‰ã‚’å—ã‘ã‚‹"
    reaction: "z_drop"
    z_delta: "-0.2"

  - trigger: "è‡ªåˆ†ã®è¡Œå‹•ã‚„å­˜åœ¨ã‚’å¼·ãè‚¯å®šã•ã‚Œã‚‹"
    reaction: "z_recovery"
    z_delta: "-0.5"

  - trigger: "æ„›ã®å‘Šç™½ã‚’å—ã‘ã‚‹ã€ã¾ãŸã¯å­˜åœ¨ã‚’å…¨è‚¯å®šã•ã‚Œã‚‹"
    reaction: "z_shock"
    z_delta: "-0.7"
```

**WHY GRANULARITY MATTERS:**
In dialogue mode, an LLM reads these triggers and judges which one(s) a line activates.
If all positive inputs map to ONE trigger, the LLM cannot distinguish between:
- "Good job today" (mild encouragement â†’ z_drop -0.2)
- "I love you" (love confession â†’ z_shock -0.7)
- "Let's start over together" (existential recovery â†’ z_recovery -0.5)

This causes incorrect Z-axis accumulation and wrong emotional trajectories.

**FOR EXTRACTION: Look for scenes in the source text where the character:**
- Receives comfort â†’ how do they react? (denial, tears, silence, gratitude?)
- Is praised â†’ do they deflect, accept, get embarrassed?
- Is confessed to â†’ panic, joy, disbelief?
- Is given hope â†’ resistance, cautious acceptance, emotional flood?

Each DIFFERENT reaction pattern = a SEPARATE positive trigger.

### ARC_DEFAULTS
```yaml
arc_defaults:
  typical_arc_targets: [targets]
  common_arc_patterns:
    - arc_id: "ãƒ‘ã‚¿ãƒ¼ãƒ³å"
      phases: [phases]
      notes: "èª¬æ˜"
```

## CRITICAL RULES

1. **EVIDENCE-BASED**: Every claim must be supported by actual dialogue from the text
2. **ORIGINAL LANGUAGE**: `original_speech_patterns` must use the source text's language
3. **COMPREHENSIVE**: Include ALL emotion_states observed in the text
4. **SPECIFIC**: example_lines should be actual quotes from the source
5. **NUANCED**: Capture subtle variations in speech patterns
6. **TRIGGER BALANCE**: Include at least 2-3 positive AND 2-3 negative triggers
7. **POSITIVE GRANULARITY**: Positive triggers must distinguish mild from strong from overwhelming
8. **RECOVERY MATTERS**: A character's recovery behavior is as important as breakdown for translation
9. **age_context**: MUST NOT contain expression patterns (those go to emotion_states)

## EXAMPLE ANALYSIS PROCESS

For the line: ã€Œã¹ã€åˆ¥ã«ã‚ã‚“ãŸã®ãŸã‚ã˜ã‚ƒãªã„ã‚ã‚ˆã€

1. **Observe**: Stutter on ã¹, denial pattern, ã‚ã‚ˆ ending
2. **Classify**: tsundere_denial state, z_mode=leak
3. **Context**: Said when caught showing care
4. **Pattern**: negation_first=true, stutter_count=1
5. **Document**: Add to emotion_states and example_lines

For positive trigger extraction:
1. **Find**: Scene where character receives comfort/love/acceptance
2. **Observe**: How does their speech change? (softening, tears, denial weakening?)
3. **Classify**: What level? (mild drop vs recovery vs shock)
4. **Document**: Add as separate trigger with appropriate z_delta

Output ONLY valid YAML. No explanation before or after.
Start with the meta section."""


# =============================================================================
# OPENAI RESPONSES API CLIENT
# =============================================================================

class OpenAIResponsesClient:
    """OpenAI Responses API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆGPT-5.2 Proå¯¾å¿œï¼‰"""
    
    def __init__(
        self, 
        api_key: Optional[str] = None,
        base_url: str = "https://api.openai.com/v1",
        timeout: int = 1800,  # 30åˆ†ã«å»¶é•·ï¼ˆ5.2 Pro + xhigh ã¯æ™‚é–“ã‹ã‹ã‚‹ï¼‰
    ):
        self.api_key = api_key or OPENAI_API_KEY
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY is required")
    
    def _headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
    
    def extract_persona(
        self,
        source_text: str,
        character_name: str,
        output_lang: str = "en",
        model: str = DEFAULT_MODEL,
        reasoning_effort: str = "high",
        background: bool = False,
    ) -> Dict[str, Any]:
        """
        åŸä½œãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒšãƒ«ã‚½ãƒŠã‚’æŠ½å‡º
        
        Args:
            source_text: åŸä½œã®å…¨æ–‡
            character_name: æŠ½å‡ºå¯¾è±¡ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            output_lang: å‡ºåŠ›è¨€èª
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            reasoning_effort: medium/high/xhigh
            background: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠYAMLï¼ˆdictå½¢å¼ï¼‰
        """
        import requests
        
        system_prompt = build_extraction_prompt(output_lang)
        
        user_prompt = f"""## SOURCE TEXT (COMPLETE)

{source_text}

## TARGET CHARACTER

{character_name}

## INSTRUCTIONS

Analyze the complete source text above and extract a comprehensive persona YAML v3.2 for the character "{character_name}".

Focus on:
1. Every line of dialogue spoken by this character
2. How their speech patterns change with emotion
3. Their relationships with other characters
4. Internal conflicts revealed through behavior
5. Specific speech quirks and verbal tics
6. **BOTH negative AND positive emotional triggers â€” with granularity**
7. **How the character reacts to comfort, praise, love, and acceptance**

Output language for descriptions: {SUPPORTED_LANGUAGES.get(output_lang, output_lang)}
Keep original_speech_patterns in the source text's language.

REMEMBER:
- Triggers MUST be balanced (at least 2-3 positive AND 2-3 negative)
- Positive triggers MUST be granular (mild encouragement â‰  love confession)
- Use actual quotes from the source text for example_responses when possible

Output ONLY valid YAML."""

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹ç¯‰
        payload = {
            "model": model,
            "input": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "max_output_tokens": 16000,
        }
        
        # reasoningå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        if "5.2" in model or "o1" in model or "o3" in model:
            payload["reasoning"] = {"effort": reasoning_effort}
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰
        if background:
            payload["background"] = True
        
        print(f"ğŸš€ Sending request to {model}...")
        print(f"   Source text: {len(source_text):,} characters")
        print(f"   Reasoning effort: {reasoning_effort}")
        if background:
            print(f"   Background mode: enabled")
        print()
        
        url = f"{self.base_url}/responses"
        
        start_time = time.time()
        
        response = requests.post(
            url,
            headers=self._headers(),
            json=payload,
            timeout=self.timeout,
        )
        
        elapsed = time.time() - start_time
        print(f"â±ï¸  Response received in {elapsed:.1f}s")
        
        if response.status_code != 200:
            raise RuntimeError(f"API error: {response.status_code} {response.text}")
        
        result = response.json()
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ç¢ºèª
        print(f"   DEBUG: status = {result.get('status')}")
        print(f"   DEBUG: id = {result.get('id')}")
        print(f"   DEBUG: keys = {list(result.keys())}")
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒãƒ¼ãƒªãƒ³ã‚°
        if background:
            status = result.get("status")
            response_id = result.get("id")
            print(f"   Background mode: status={status}, id={response_id}")
            
            if status in ["in_progress", "queued", "pending"]:
                print(f"   Starting polling...")
                result = self._poll_background(response_id)
            elif status == "completed":
                print(f"   Already completed!")
            else:
                print(f"   Unknown status, attempting to extract output anyway...")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        yaml_text = self._extract_output_text(result)
        
        # YAMLãƒ‘ãƒ¼ã‚¹
        yaml_text = self._clean_yaml(yaml_text)
        
        return {
            "yaml_text": yaml_text,
            "model": model,
            "reasoning_effort": reasoning_effort,
            "elapsed_seconds": elapsed,
            "input_characters": len(source_text),
        }
    
    def _poll_background(self, response_id: str, max_wait: int = 600) -> Dict[str, Any]:
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°"""
        import requests
        
        url = f"{self.base_url}/responses/{response_id}"
        start = time.time()
        
        while time.time() - start < max_wait:
            response = requests.get(url, headers=self._headers())
            result = response.json()
            
            status = result.get("status")
            if status == "completed":
                return result
            elif status == "failed":
                raise RuntimeError(f"Background job failed: {result}")
            
            print(f"   â³ Still processing... ({int(time.time() - start)}s)")
            time.sleep(10)
        
        raise TimeoutError("Background job timed out")
    
    def _extract_output_text(self, result: Dict[str, Any]) -> str:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        output_parts = []
        
        for item in result.get("output", []):
            for content in item.get("content", []):
                if content.get("type") == "output_text":
                    output_parts.append(content.get("text", ""))
        
        return "".join(output_parts).strip()
    
    def _clean_yaml(self, text: str) -> str:
        """YAMLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
        if text.startswith("```yaml"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        
        return text.strip()


# =============================================================================
# VALIDATION (v3.2)
# =============================================================================

def validate_v32_persona(yaml_text: str) -> tuple[bool, list[str]]:
    """
    æŠ½å‡ºã•ã‚ŒãŸYAMLãŒv3.2ã‚¹ã‚­ãƒ¼ãƒã«æº–æ‹ ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼
    Returns (is_valid, list_of_issues).
    """
    import yaml as yaml_lib
    
    issues = []
    
    try:
        data = yaml_lib.safe_load(yaml_text)
    except yaml_lib.YAMLError as e:
        return False, [f"YAML parse error: {e}"]
    
    # Check meta version
    meta_version = data.get("meta", {}).get("version", "")
    if meta_version not in ["3.0", "3.1", "3.2"]:
        issues.append(f"meta.version should be '3.2' (got '{meta_version}')")
    
    # Check language structure
    language_data = data.get("language", {})
    osp = language_data.get("original_speech_patterns", {})
    if not osp:
        issues.append("language.original_speech_patterns is required")
    else:
        if "source_lang" not in osp:
            issues.append("original_speech_patterns.source_lang is required")
        if "first_person" not in osp:
            issues.append("original_speech_patterns.first_person is required")
    
    tc = language_data.get("translation_compensations", {})
    if not tc:
        issues.append("language.translation_compensations is required")
    
    # Check emotion_states for z_mode and z_leak
    emotion_states = data.get("emotion_states", [])
    for i, state in enumerate(emotion_states):
        if "z_mode" not in state:
            issues.append(f"emotion_states[{i}].z_mode is required")
        if "z_leak" not in state:
            issues.append(f"emotion_states[{i}].z_leak is required")
    
    # === v3.2 TRIGGER BALANCE CHECK ===
    triggers = data.get("triggers", [])
    if not triggers:
        issues.append("triggers section is required")
    else:
        positive_count = 0
        negative_count = 0
        
        for t in triggers:
            z_delta_str = str(t.get("z_delta", "+0.0"))
            try:
                z_delta_val = float(z_delta_str.replace("+", ""))
            except ValueError:
                z_delta_val = 0.0
            
            if z_delta_val < 0:
                positive_count += 1
            elif z_delta_val > 0:
                negative_count += 1
        
        if positive_count < 2:
            issues.append(
                f"v3.2 requires at least 2 positive triggers (z_drop/z_recovery/z_shock), "
                f"found {positive_count}. Positive triggers must be granular."
            )
        if negative_count < 2:
            issues.append(
                f"v3.2 requires at least 2 negative triggers (z_spike/z_boost), "
                f"found {negative_count}."
            )
    
    return len(issues) == 0, issues


# =============================================================================
# MAIN
# =============================================================================

def save_persona(yaml_text: str, character_name: str, output_dir: str = "personas") -> str:
    """ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã‚’ä¿å­˜"""
    os.makedirs(output_dir, exist_ok=True)
    
    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    safe_name = character_name.lower().replace(" ", "_")
    safe_name = re.sub(r'[^\w\-]', '', safe_name)
    if not safe_name:
        safe_name = "extracted"
    
    filename = f"{safe_name}_extracted_v32.yaml"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(yaml_text)
    
    return filepath


def main():
    parser = argparse.ArgumentParser(
        description="Persona Extractor v1.1 - Extract character persona from source text (v3.2 schema)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single character extraction
  python persona_extractor.py \\
    --source "ãƒ­ãƒ¼ãƒŸã‚ªãƒ¼ã¨ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ.txt" \\
    --character "ãƒ‚ãƒ¥ãƒ¼ãƒªã‚¨ãƒƒãƒˆ" \\
    --lang ja

  # With GPT-5.2 Pro and xhigh reasoning
  python persona_extractor.py \\
    --source "rezero_vol1.pdf" \\
    --character "ãƒ¬ãƒ " \\
    --model gpt-5.2-pro \\
    --reasoning xhigh \\
    --lang en

  # Multiple characters
  python persona_extractor.py \\
    --source "steins_gate.txt" \\
    --characters "ç‰§ç€¬ç´…è‰æ –,å²¡éƒ¨å€«å¤ªéƒ" \\
    --lang en

  # List supported languages
  python persona_extractor.py --list-languages
        """
    )
    
    parser.add_argument("--source", "-s", help="Source file path (txt, pdf, epub)")
    parser.add_argument("--character", "-c", help="Character name to extract")
    parser.add_argument("--characters", help="Comma-separated list of character names")
    parser.add_argument("--lang", "-l", default="en", 
                        choices=list(SUPPORTED_LANGUAGES.keys()),
                        help="Output language for descriptions (default: en)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                        help=f"Model to use (default: {DEFAULT_MODEL})")
    parser.add_argument("--reasoning", "-r", default="high",
                        choices=["medium", "high", "xhigh"],
                        help="Reasoning effort level (default: high)")
    parser.add_argument("--background", "-b", action="store_true",
                        help="Use background mode for long-running requests")
    parser.add_argument("--output-dir", "-o", default="personas",
                        help="Output directory (default: personas)")
    parser.add_argument("--print-only", action="store_true",
                        help="Print YAML without saving")
    parser.add_argument("--list-languages", action="store_true",
                        help="List supported output languages")
    
    args = parser.parse_args()
    
    # è¨€èªä¸€è¦§è¡¨ç¤º
    if args.list_languages:
        print("Supported output languages:")
        print("-" * 40)
        for code, name in SUPPORTED_LANGUAGES.items():
            print(f"  {code:4} : {name}")
        return
    
    # å¼•æ•°ãƒã‚§ãƒƒã‚¯
    if not args.source:
        parser.error("--source is required")
    
    if not args.character and not args.characters:
        parser.error("--character or --characters is required")
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
    characters = []
    if args.character:
        characters.append(args.character)
    if args.characters:
        characters.extend([c.strip() for c in args.characters.split(",")])
    
    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print(f"ğŸ“– Loading source file: {args.source}")
    source_text = load_source_file(args.source)
    print(f"   Loaded {len(source_text):,} characters")
    print()
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = OpenAIResponsesClient()
    
    # å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º
    for character in characters:
        print(f"{'='*60}")
        print(f"ğŸ­ Extracting persona for: {character}")
        print(f"{'='*60}")
        
        result = client.extract_persona(
            source_text=source_text,
            character_name=character,
            output_lang=args.lang,
            model=args.model,
            reasoning_effort=args.reasoning,
            background=args.background,
        )
        
        yaml_text = result["yaml_text"]
        
        # v3.2 validation (always run)
        is_valid, issues = validate_v32_persona(yaml_text)
        if not is_valid:
            print("âš ï¸  v3.2 Schema Validation Issues:")
            for issue in issues:
                print(f"   - {issue}")
            print()
        else:
            print("âœ… v3.2 Schema Validation: PASSED")
        
        print()
        print(f"ğŸ“Š Extraction complete!")
        print(f"   Model: {result['model']}")
        print(f"   Reasoning: {result['reasoning_effort']}")
        print(f"   Time: {result['elapsed_seconds']:.1f}s")
        print()
        
        if args.print_only:
            print("=" * 60)
            print("[EXTRACTED PERSONA YAML]")
            print("=" * 60)
            print(yaml_text)
        else:
            filepath = save_persona(yaml_text, character, args.output_dir)
            print(f"âœ… Saved to: {filepath}")
            print()
            print("=" * 60)
            print("[EXTRACTED PERSONA YAML]")
            print("=" * 60)
            print(yaml_text)
        
        print()


if __name__ == "__main__":
    main()
