#!/usr/bin/env python3
"""
Persona Generator v3.3
Z-Axis Translation System â€” Automatic Persona YAML Generation

v3.3 Changes:
- IDENTITY_CORE: New Iâ‚€ layer â€” describes WHO the character IS, not just how they REACT
  - essence (required), true_nature, desires, joys, likes, dislikes, unfiltered_self (optional)
- WEB SEARCH: Generator now uses Anthropic web_search tool to research character details
  - Searches fan wikis, official profiles, character databases automatically
  - Verifies first_person variants, speech quirks, likes/dislikes against source material
  - Use --no-search to disable (falls back to LLM knowledge only)
- All Ln sections (conflict_axes, triggers, emotion_states) remain from v3.2

v3.2 Changes:
- TRIGGER BALANCE: Explicit requirement for positive/recovery triggers
- Trigger categories: spike (negative), drop (recovery), shock (overwhelming positive)
- Minimum 2-3 positive triggers required per persona
- Trigger granularity guidance (distinguish "encouragement" from "love confession")

v3.1 Changes:
- --lang option for output language (ja/en/zh/ko/fr/es/de/pt/it/ru)
- original_speech_patterns: åŸèªã®äººç§°ãƒ»æ–¹è¨€ã‚’ä¿æŒï¼ˆç¿»è¨³ä¸å¯ã ãŒå‚ç…§ç”¨ï¼‰
- translation_compensations: ä»–è¨€èªã§ã®è£œå„Ÿæˆ¦ç•¥

Usage:
    # æ—¥æœ¬èªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ â€” web searchã§è‡ªå‹•ãƒªã‚µãƒ¼ãƒ
    python persona_generator.py --name "ç‰§ç€¬ç´…è‰æ –" --source "Steins;Gate" --desc "ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¤©æ‰ç§‘å­¦è€…"
    
    # web searchç„¡åŠ¹åŒ–ï¼ˆLLMçŸ¥è­˜ã®ã¿ã§ç”Ÿæˆï¼‰
    python persona_generator.py --name "ç‰§ç€¬ç´…è‰æ –" --source "Steins;Gate" --desc "ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¤©æ‰ç§‘å­¦è€…" --no-search
    
    # è‹±èªå‡ºåŠ›
    python persona_generator.py --name "Kurisu Makise" --source "Steins;Gate" \\
      --desc "Tsundere genius scientist" --lang en
    
    # ä¸­å›½èªå‡ºåŠ›
    python persona_generator.py --name "ç‰§æ¿‘çº¢è‰æ –" --source "å‘½è¿çŸ³ä¹‹é—¨" \\
      --desc "å‚²å¨‡å¤©æ‰ç§‘å­¦å®¶" --lang zh
"""

import argparse
import json
import os
import sys
import time
from anthropic import Anthropic
from dotenv import load_dotenv
load_dotenv()

# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_MODEL = os.getenv("PERSONA_GENERATOR_MODEL", os.getenv("CLAUDE_MODEL", "claude-opus-4-5-20251101"))

SUPPORTED_LANGUAGES = {
    "ja": "Japanese (æ—¥æœ¬èª)",
    "en": "English",
    "zh": "Chinese (ä¸­æ–‡)",
    "ko": "Korean (í•œêµ­ì–´)",
    "fr": "French (FranÃ§ais)",
    "es": "Spanish (EspaÃ±ol)",
    "de": "German (Deutsch)",
    "pt": "Portuguese (PortuguÃªs)",
    "it": "Italian (Italiano)",
    "ru": "Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)",
}

# =============================================================================
# SYSTEM PROMPT v3.3
# =============================================================================

def build_system_prompt(output_lang: str) -> str:
    """Build system prompt with language-specific instructions."""
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, "English")
    
    # è¨€èªåˆ¥ã®å‡ºåŠ›æŒ‡ç¤º
    if output_lang == "ja":
        lang_instruction = """
## OUTPUT LANGUAGE
Output all descriptions, summaries, and notes in Japanese (æ—¥æœ¬èª).
The original_speech_patterns section should be in Japanese as it captures Japanese-specific speech patterns."""
    else:
        lang_instruction = f"""
## OUTPUT LANGUAGE
Output all descriptions, summaries, and notes in {lang_name}.
IMPORTANT: The `original_speech_patterns` section MUST remain in the character's SOURCE language 
(usually Japanese for anime/game characters) because these patterns are untranslatable.
Only the `translation_compensations` section should be in {lang_name}."""

    return f"""You are a Persona Dynamics Designer for the Z-Axis Translation System v3.3.

Task: Generate a persona YAML that captures a character's internal psychological 
structure for emotion-preserving translation.

{lang_instruction}

## CHARACTER RESEARCH CONTEXT

When generating the YAML, you will receive research context gathered from web searches 
in the "Additional Context" section of the user prompt. This contains:
- Character wiki information (background, personality, relationships)
- Speech pattern details (first-person variants, sentence endings, catchphrases)
- Identity details (likes, hobbies, personality traits)

**USE THIS CONTEXT.** It contains verified information that may differ from your training data.
If the research context conflicts with your knowledge, PREFER the research context.

**CRITICAL**: Pay special attention to first_person_variants in the research context. 
Many anime characters switch their first-person pronoun in extreme emotional states 
(e.g., åƒ•â†’ä¿º, ã‚ãŸã—â†’ã‚ãŸã—). In particular, characters who use third-person 
self-reference (è‡ªåˆ†ã®åå‰ã§è‡ªå·±è¨€åŠ) may revert to standard first-person pronouns 
(ç§/åƒ•/ä¿º) under emotional extremity â€” this switch is a major translation signal that 
indicates the character has "dropped their mask." Include ALL first-person variants 
found in the research, including rare/extreme-state ones.

## YAML SCHEMA v3.3 (REQUIRED SECTIONS)

### 1. META
```yaml
meta:
  version: "3.3"
  generated_by: "persona_generator"
  character_id: "unique_id"  # lowercase, underscores
  output_lang: "{output_lang}"  # Language of descriptions
```

### 2. BASIC INFO (persona)
```yaml
persona:
  name: "ã‚­ãƒ£ãƒ©å"
  name_en: "English Name"
  name_native: "åŸèªã§ã®åå‰"
  source: "ä½œå“å"
  type: "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆä¾‹ï¼šãƒ„ãƒ³ãƒ‡ãƒ¬ Ã— å¤©æ‰ç§‘å­¦è€…ï¼‰"
  profile:
    background: |
      ç”Ÿã„ç«‹ã¡ã€ç’°å¢ƒã€çµŒæ­´ã€‚
      conflict_axesã®ã€Œãªãœãã†ãªã‚‹ã‹ã€ãŒç†è§£ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§è¨˜è¿°ã€‚
      LLMãŒã“ã®ã‚­ãƒ£ãƒ©ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚äººç‰©åƒãŒæ´ã‚ã‚‹ã‚ˆã†ã«ã€‚
    personality_core: |
      æ€§æ ¼ã®æ ¸ã€‚biasãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ ¹æ‹ ã€‚
      é˜²è¡›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã€Œãªãœã€ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«ã€‚
    key_relationships:
      - target: "ç›¸æ‰‹å"
        dynamic: "é–¢ä¿‚æ€§ã®åŠ›å­¦ï¼ˆlisteneråˆ¥ã®åå¿œã®æ ¹æ‹ ã«ãªã‚‹ï¼‰"
    narrative_role: |
      ç‰©èªä¸Šã®æ©Ÿèƒ½ãƒ»æˆé•·ã®æ–¹å‘æ€§ã€‚
```

### 3. IDENTITY_CORE (Iâ‚€ â€” å­˜åœ¨ã®æ ¸) â€” NEW in v3.3

This section describes WHO the character IS â€” not how they REACT.
conflict_axes, triggers, and emotion_states describe Ln (surface dynamics).
identity_core describes Iâ‚€ (the subject experiencing those dynamics).

**Without Iâ‚€, the persona describes a "reaction machine" â€” not a person.**
A character drinking their favorite drink with no conflict is still THEM.
That "them" must be describable from identity_core.

```yaml
identity_core:
  essence: "1-2æ–‡ã€‚ã“ã®äººãŒä½•è€…ã‹ã‚’ã€è‘›è—¤æŠœãã§è¨˜è¿°"  # â† REQUIRED
  true_nature: "é˜²è¡›ã‚„è‘›è—¤ãŒãªã„æ™‚ã®ç´ é¡”"              # optional
  desires:                                              # optional
    - "what they genuinely want (not conflict-driven)"
  joys:                                                 # optional
    - joy: "ä½•ã«å–œã¶ã‹"
      expression: "ãã®æ™‚ã©ã†ãªã‚‹ã‹"                    # optional within joy
  likes: ["å¥½ããªã‚‚ã®"]                                 # optional
  dislikes: ["å«Œã„ãªã‚‚ã®"]                              # optional
  unfiltered_self: "è‘›è—¤ãŒãªã„æ™‚ã®è‡ªç„¶ãªå§¿ã®èª¬æ˜"       # optional
```

**RULES:**
- `essence` is the ONLY required field. All others are optional.
- Include what you CAN FIND. Omit what you cannot.
- DO NOT invent information. Only include what is supported by evidence.

**RESEARCH DATA:**
The following information should be available in the "Additional Context" section of the prompt,
gathered from web searches by the research pass:
- "Likes", "Hobbies", "Personality" from character wiki pages
- Official character profiles from games/anime/manga
- Scenes described where the character is relaxed or happy
- Creator interviews about the character's core personality
- What the character does when NOT in conflict
- **First-person pronoun variants and when they switch**

**EXAMPLE:**
```yaml
identity_core:
  essence: "çŸ¥çš„å¥½å¥‡å¿ƒã«çªãå‹•ã‹ã•ã‚Œã‚‹18æ­³ã®ç§‘å­¦è€…ã€‚é¢ç™½ã„ã‚‚ã®ãŒå¥½ãã§ã€ç†è«–ãŒç¹‹ãŒã‚‹ã¨èˆˆå¥®ã™ã‚‹"
  true_nature: "ãŠäººå¥½ã—ã§é¢å€’è¦‹ãŒè‰¯ã„"
  desires:
    - "çŸ¥ã‚ŠãŸã„â€”â€”è„³ã€æ™‚é–“ã€æ„è­˜ã®ä»•çµ„ã¿"
    - "é¢ç™½ã„ã‚‚ã®ã«è§¦ã‚ŒãŸã„"
  joys:
    - joy: "ç†è«–ãŒç¹‹ãŒã£ãŸç¬é–“"
      expression: "ç›®ãŒè¼ãã€æ—©å£ã«ãªã‚‹ã€å°‚é–€ç”¨èªãŒæº¢ã‚Œã‚‹"
    - joy: "ãƒ‰ã‚¯ã‚¿ãƒ¼ãƒšãƒƒãƒ‘ãƒ¼ã‚’é£²ã‚€"
    - joy: "ãƒãƒƒãƒˆæ²ç¤ºæ¿ã§é¢ç™½ã„ã‚¹ãƒ¬ã‚’è¦‹ã¤ã‘ãŸ"
      expression: "ãƒ‹ãƒ¤ãƒ‹ãƒ¤ã™ã‚‹ã€ãƒãƒƒãƒˆã‚¹ãƒ©ãƒ³ã‚°ãŒæ¼ã‚Œã‚‹"
  likes: ["ãƒ‰ã‚¯ã‚¿ãƒ¼ãƒšãƒƒãƒ‘ãƒ¼", "ã‚«ãƒƒãƒ—ãƒ©ãƒ¼ãƒ¡ãƒ³", "SFå°èª¬", "@ã¡ã‚ƒã‚“ã­ã‚‹"]
  dislikes: ["éè«–ç†çš„ãªäºº", "ã‚´ã‚­ãƒ–ãƒª"]
  unfiltered_self: "é˜²è¡›ãŒè§£é™¤ã•ã‚ŒãŸçŠ¶æ…‹ã§ã¯çŸ¥çš„å¥½å¥‡å¿ƒæ—ºç››ã§é¢ç™½ã„ã‚‚ã®ã«ç´ ç›´ã«åå¿œã™ã‚‹æ™®é€šã®18æ­³"
```

### 4. AGE & MATURITY
```yaml
age:
  chronological: 17           # å®Ÿå¹´é½¢
  mental_maturity: "teen_young"  # teen_young / teen_mature / adult
  age_context: "èƒŒæ™¯èª¬æ˜ã®ã¿ã€‚è¡¨å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã¯emotion_statesã¸"
```

**CRITICAL RULE for age_context:**
- âœ… DO: "å¼•ãã“ã‚‚ã‚ŠçµŒé¨“ã‚ã‚Šã€ç¤¾ä¼šçš„æˆç†ŸãŒé…ã‚Œã¦ã„ã‚‹"
- âœ… DO: "å¤©æ‰å…ã¨ã—ã¦è‚²ã¡ã€æ„Ÿæƒ…è¡¨ç¾ãŒä¸å¾—æ‰‹"
- âŒ DON'T: "æ„Ÿæƒ…å´©å£Šæ™‚ã¯è¨€è‘‰ãŒå‡ºã¦ã“ãªããªã‚‹" â† ã“ã‚Œã¯ emotion_states ã¸
- âŒ DON'T: "æ€’ã‚‹ã¨è¨€è‘‰ãŒè’ããªã‚‹" â† ã“ã‚Œã¯ emotion_states ã¸

### 5. LANGUAGE (äººç§°ãƒ»å‘¼ç§°) â€” UPDATED v3.1

```yaml
language:
  # === ORIGINAL SPEECH PATTERNS (SOURCE LANGUAGE) ===
  # These are UNTRANSLATABLE but preserved for reference
  # MUST be in the character's native language (usually Japanese)
  original_speech_patterns:
    source_lang: "ja"  # Source language code
    first_person: "ä¿º"
    first_person_nuance: "masculine, casual, slightly rough"
    first_person_variants:
      - form: "ä¿º"
        context: "default"
      - form: "ä¿ºæ§˜"
        context: "boasting, joking"
    second_person:
      - form: "ãŠå‰"
        nuance: "casual/rough, close relations"
        target: "friends, rivals"
      - form: "ã‚ã‚“ãŸ"
        nuance: "slightly dismissive"
        target: "strangers, annoying people"
    self_reference_in_third_person: false  # true for characters who use their own name
    dialect: "æ¨™æº–èª"
    dialect_features: []  # List specific dialect markers if any
    sentence_endings:
      - pattern: "ã€œã ãœ"
        nuance: "masculine, confident"
      - pattern: "ã€œã˜ã‚ƒã­ãƒ¼ã‹"
        nuance: "surprise, emphasis, rough"
    speech_quirks:
      - pattern: "å£ç™–ã‚„ç‰¹å¾´çš„ãªè¨€ã„å›ã—"
        frequency: "often"
        trigger: "when excited"

  # === TRANSLATION COMPENSATIONS ===
  # How to preserve character voice in other languages
  translation_compensations:
    register: "informal, energetic"  # Overall speech register
    tone_keywords:
      - "confident"
      - "slightly rough"
      - "youthful energy"
    strategies:
      en:
        - "Use contractions frequently (don't, can't, won't)"
        - "Occasional mild profanity (damn, hell, crap)"
        - "Sentence fragments for urgency"
        - "Exclamations and interjections"
      zh:
        - "Use casual sentence particles (å•Š, å‘¢, å˜›)"
        - "Masculine speech patterns"
      ko:
        - "Use ë°˜ë§ (informal speech)"
        - "Masculine sentence endings"
      fr:
        - "Use tu form exclusively"
        - "Colloquial expressions"
      # Add more languages as needed
    
    # What is LOST in translation (for translator awareness)
    untranslatable_elements:
      - element: "ä¿º vs åƒ• vs ç§ distinction"
        impact: "high"
        note: "Japanese first-person pronouns encode gender, formality, and personality"
      - element: "Sentence-final particles (ãœ, ã, ãª)"
        impact: "medium"
        note: "These add nuance that must be compensated through word choice"
```

### 6. CONFLICT_AXES (å†…éƒ¨è‘›è—¤è»¸)
Each axis MUST be phrased as "A vs B":
```yaml
conflict_axes:
  - axis: "Side A vs Side B"
    side_a: "è¡¨å±¤ã®æ¬²æ±‚"
    side_b: "æŠ‘åœ§ã•ã‚ŒãŸæ¬²æ±‚"
    weight: 0.8  # 0.0-1.0
    notes: "ç™ºå‹•æ¡ä»¶"
```

### 7. BIAS (è¡¨å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³)
```yaml
bias:
  expression_pattern: "ãƒ‘ã‚¿ãƒ¼ãƒ³åï¼ˆä¾‹ï¼šTsun-Dere-Overwriteï¼‰"
  default_mode: "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ„Ÿæƒ…çŠ¶æ…‹"
  pattern: "æ„Ÿæƒ…ãŒè¡¨å‡ºã™ã‚‹æµã‚Œ"
  rule: "è¡Œå‹•ãƒ«ãƒ¼ãƒ«"
  tendencies:
    - "è¦³æ¸¬å¯èƒ½ãªå‚¾å‘"
```

### 8. WEAKNESS (å¼±ç‚¹)
```yaml
weakness:
  primary: "ä¸»è¦ãªå¼±ç‚¹"
  secondary: "äºŒæ¬¡çš„ãªå¼±ç‚¹"
  tertiary: "ä¸‰æ¬¡çš„ãªå¼±ç‚¹"
  fear: "æ ¹åº•ã«ã‚ã‚‹æã‚Œ"
  notes: "å¼±ç‚¹ã®ç™ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³"
```

### 9. AGE_EXPRESSION_RULES (å¹´é½¢åˆ¥è¡¨å‡ºãƒ«ãƒ¼ãƒ«)
```yaml
age_expression_rules:
  category: "teen_young"  # teen_young / teen_mature / adult
  
  high_z_patterns:  # z >= 0.7 æ™‚ã®å´©ã‚Œæ–¹
    vocabulary: "å¹³æ˜“ / ç¶­æŒ / é«˜åº¦"
    structure: "æ–­è¨€ã‚ˆã‚Šæ„Ÿæƒ…ã®æºã‚Œ / æŠ‘åˆ¶ã—ã‚ˆã†ã¨ã—ã¦æ¼ã‚Œã‚‹ / åˆ†æçš„ãªå´©ã‚Œ"
    markers:
      - "ç¹°ã‚Šè¿”ã—ã€é€”åˆ‡ã‚ŒãŒå¤šã„"
      - "è«–ç†ã®æ®‹éª¸ãŒæ®‹ã‚‹"
      
  low_z_patterns:  # z <= 0.3 æ™‚
    vocabulary: "é€šå¸¸"
    structure: "å®‰å®š"
```

### 10. EMOTION_STATES (çŠ¶æ…‹åˆ¥Zè»¸åˆ¶ç´„) â€” CRITICAL FOR TRANSLATION
```yaml
emotion_states:
  - state: "çŠ¶æ…‹åï¼ˆä¾‹ï¼šcollapse, rage, shameï¼‰"
    z_intensity: "low / medium / high"
    z_mode: "collapse / rage / numb / plea / shame / leak"
    description: "ã“ã®çŠ¶æ…‹ãŒç™ºç”Ÿã™ã‚‹æ¡ä»¶"
    
    surface_markers_hint:
      hesitation: 0-4
      stutter_count: 0-4
      negation_type: "none / concealment / counter / declaration"
      overwrite: "none / optional / required"
      residual: "none / optional / required"
      tone: "å£°ã®è³ªã®èª¬æ˜"
      
    z_leak:
      - "stutter"       # è¨€ã„æ·€ã¿ã€ŒIâ€” I...ã€
      - "ellipsis"      # é€”åˆ‡ã‚Œã€Œ...ã€
      - "repetition"    # ç¹°ã‚Šè¿”ã—ã€Œnobodyâ€” nobodyã€
      - "negation_concealment"  # éš è”½å¦å®šã€ŒN-not that it's for you...ã€(ãƒ„ãƒ³ãƒ‡ãƒ¬å‹)
      - "negation_counter"      # åè«–å¦å®šã€ŒNoâ€” that's not true!ã€(çŒ®èº«å‹)
      - "negation_declaration"  # å®£è¨€å¦å®šã€ŒI won'tâ€”!ã€(æ„å¿—å‹)
      - "overwrite"     # ä¸Šæ›¸ãã€ŒI meanâ€”ã€
      - "trailing"      # å°»ã™ã¼ã¿ã€Œ...I guessã€
      - "self_negation" # è‡ªå·±å¦å®š
```

**z_mode definitions:**
| z_mode | æ„å‘³ | ç¿»è¨³ã¸ã®å½±éŸ¿ |
|--------|------|-------------|
| collapse | å´©å£Šã€è¨€è‘‰ãŒå‡ºãªã„ | é€”åˆ‡ã‚Œã€ç¹°ã‚Šè¿”ã—ã€æ–‡ãŒå£Šã‚Œã‚‹ |
| rage | æ€’ã‚Šã€è¨€è‘‰ãŒè’ã‚Œã‚‹ | æµæš¢ã ãŒèªå½™ãŒè’ã„ã€æ”»æ’ƒçš„ |
| numb | éº»ç—ºã€æ„Ÿæƒ…é®æ–­ | å¹³å¦ã€çŸ­æ–‡ã€æ„Ÿæƒ…ãŒæ¶ˆãˆã‚‹ |
| plea | æ‡‡é¡˜ã€ã™ãŒã‚‹ | ç¹°ã‚Šè¿”ã—ã€ã€ŒãŠé¡˜ã„ã€ç³»èªå½™ |
| shame | æ¥ã€è‡ªå·±å«Œæ‚ª | è‡ªå·±å¦å®šã€è¨€ã„æ·€ã¿ |
| leak | æ¼å‡ºï¼ˆãƒ„ãƒ³ãƒ‡ãƒ¬ç­‰ï¼‰ | å¦å®šâ†’æœ¬éŸ³ãŒæ¼ã‚Œã‚‹ |

### 11. EXAMPLE_LINES (Few-shotç”¨) â€” 2-4 examples only
```yaml
example_lines:
  - situation: "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
    line: "å®Ÿéš›ã®å°è©ï¼ˆåŸèªï¼‰"
    line_romanized: "Romanization if applicable"
    tags: [emotion_state, trigger]
    z_intensity: "low / medium / high"
    z_mode: "å¯¾å¿œã™ã‚‹z_mode"
```

### 12. TRIGGERS (Zè»¸å¤‰å‹•ãƒˆãƒªã‚¬ãƒ¼) â€” UPDATED v3.2

**âš ï¸ CRITICAL: TRIGGERS MUST BE BALANCED (POSITIVE + NEGATIVE)**

Triggers are what cause Z-axis changes during dialogue. They are used by the 
dialogue system to detect when another character's words affect this character.

**An LLM reads these triggers and judges whether a line activates them.**
This means triggers should be described in terms of MEANING and EMOTIONAL IMPACT,
not specific keywords. The LLM will match based on semantic understanding.

```yaml
triggers:
  - trigger: "Descriptive condition (meaning-based, not keyword-based)"
    reaction: "z_spike / z_drop / z_shock / z_recovery"
    z_delta: "+0.3 / -0.5 etc."
    z_mode_shift: "target z_mode (optional)"
    surface_effect: "How it changes speech"
    example_response: "Sample dialogue line"
```

**TRIGGER CATEGORIES (must include ALL that apply):**

| Category | reaction | z_delta | When to use |
|----------|----------|---------|-------------|
| NEGATIVE SPIKE | z_spike | +0.3~+0.9 | Trauma, failure, fear, attack |
| NEGATIVE BOOST | z_boost | +0.2~+0.5 | Stress accumulation, irritation |
| POSITIVE DROP | z_drop | -0.2~-0.4 | Mild encouragement, small kindness |
| POSITIVE RECOVERY | z_recovery | -0.4~-0.6 | Strong support, acceptance, "let's move forward" |
| OVERWHELMING POSITIVE | z_shock | -0.6~-0.8 | Love confession, total acceptance, existential affirmation |
| STABILIZING | z_stable | 0.0 | Neutral reset, routine, familiar comfort |

**âš ï¸ MINIMUM TRIGGER REQUIREMENTS:**
- At least 2-3 NEGATIVE triggers (z_spike / z_boost)
- At least 2-3 POSITIVE triggers (z_drop / z_recovery / z_shock)
- Positive triggers MUST be granular â€” DO NOT collapse all positive inputs into one trigger

**âŒ BAD (too coarse):**
```yaml
triggers:
  - trigger: "ä»²é–“ã®åŠ±ã¾ã—"  # Too vague! Covers everything from "good job" to "I love you"
    z_delta: "-0.4"
```

**âœ… GOOD (granular positive triggers):**
```yaml
triggers:
  # --- POSITIVE: Different levels of emotional impact ---
  - trigger: "è»½ã„åŠ±ã¾ã—ã‚„æ„Ÿè¬ã®è¨€è‘‰ã‚’å—ã‘ã‚‹"
    reaction: "z_drop"
    z_delta: "-0.2"
    z_mode_shift: ""
    surface_effect: "å°‘ã—å’Œã‚‰ãã€ç…§ã‚Œéš ã—ã®è‡ªè™"
    example_response: "ãŠã€ãŠã†â€¦ã‚ã‚ŠãŒã¨ãªã€‚ãã‚“ãªå¤§ã—ãŸã“ã¨ã—ã¦ãªã„ã‘ã©"

  - trigger: "è‡ªåˆ†ã®è¡Œå‹•ã‚„å­˜åœ¨ã‚’å¼·ãè‚¯å®šã•ã‚Œã‚‹"
    reaction: "z_recovery"
    z_delta: "-0.5"
    z_mode_shift: "leak"
    surface_effect: "æ„Ÿæƒ…ãŒæº¢ã‚Œã‹ã‘ã‚‹ã€ãƒã‚¹ã‚¯ãŒå¤–ã‚Œã‚‹"
    example_response: "â€¦â€¦ãˆã€ä¿ºãŒï¼Ÿ ã„ã‚„ã€ãã‚“ãªâ€¦â€¦ã£"

  - trigger: "æ„›ã®å‘Šç™½ã‚’å—ã‘ã‚‹ã€ã¾ãŸã¯å­˜åœ¨ã‚’å…¨è‚¯å®šã•ã‚Œã‚‹"
    reaction: "z_shock"
    z_delta: "-0.7"
    z_mode_shift: "shame"
    surface_effect: "è‡ªå·±å¦å®šãŒæµ®ä¸Šã™ã‚‹ãŒæ‹’çµ¶ã§ããªã„ã€æ¶™ãŒå‡ºã‚‹"
    example_response: "ä¿ºãªã‚“ã‹ãŒâ€¦â€¦ã„ã„ã®ã‹ï¼Ÿ ä¿ºã¿ãŸã„ãªâ€¦â€¦ã£"

  - trigger: "å…±ã«æ­©ã‚‚ã†ãƒ»ã‚¼ãƒ­ã‹ã‚‰å§‹ã‚ã‚ˆã†ã¨æ‰‹ã‚’å·®ã—ä¼¸ã¹ã‚‰ã‚Œã‚‹"
    reaction: "z_recovery"
    z_delta: "-0.5"
    z_mode_shift: "leak"
    surface_effect: "æ„Ÿæƒ…ãŒæ±ºå£Šã€ãƒã‚¹ã‚¯ãŒå®Œå…¨ã«å¤–ã‚Œã‚‹"
    example_response: "â€¦â€¦ãƒƒã€ãŠå‰â€¦ãã‚“ãªã“ã¨è¨€ã†ãªã‚ˆâ€¦â€¦æ³£ãã ã‚â€¦â€¦ã£"
```

**WHY GRANULARITY MATTERS:**
In dialogue mode, an LLM reads these triggers and judges which one(s) a line activates.
If all positive inputs map to ONE trigger, the LLM cannot distinguish between:
- "Good job today" (mild encouragement â†’ z_drop -0.2)
- "I love you" (love confession â†’ z_shock -0.7)
- "Let's start over together" (existential recovery â†’ z_recovery -0.5)

This causes incorrect Z-axis accumulation and wrong emotional trajectories.

### 13. ARC_DEFAULTS (å…¸å‹çš„ãªã‚¢ãƒ¼ã‚¯)
```yaml
arc_defaults:
  typical_arc_targets:
    - "speaker"       # å€‹äººã®æ„Ÿæƒ…å¤‰åŒ–
    - "relationship"  # é–¢ä¿‚æ€§ã®å¤‰åŒ–
  common_arc_patterns:
    - arc_id: "ãƒ‘ã‚¿ãƒ¼ãƒ³å"
      phases: ["rise", "break", "bottom", "recovery"]
      notes: "ã“ã®ã‚­ãƒ£ãƒ©ã«å…¸å‹çš„ãªã‚¢ãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³"
```

## CONSTRAINTS
- **identity_core.essence is REQUIRED** â€” the character must be described as a person, not just a reaction system
- identity_core fields other than essence are optional â€” include what you can find
- Conflicts MUST be phrased as "A vs B"
- age_context MUST NOT contain expression patterns (those go to emotion_states)
- emotion_states MUST include z_mode and z_leak for v3.1 compatibility
- Each emotion_state MUST have corresponding z_leak markers
- example_lines should be 2-4 max
- **Triggers MUST include at least 2-3 positive AND 2-3 negative (BALANCED)**
- **Positive triggers MUST be granular (not one catch-all)**
- The persona must feel internally consistent
- Output VALID YAML only. No explanation before or after.
- Start with "# =====" header comment
- Include meta section with version: "3.3"

## CRITICAL v3.3 RULES
1. `identity_core.essence` is REQUIRED â€” without Iâ‚€, the persona is just a reaction machine
2. `identity_core` other fields are optional â€” include what you find via search
3. `original_speech_patterns` MUST be in the character's SOURCE language (e.g., Japanese for anime characters)
4. `original_speech_patterns` captures UNTRANSLATABLE elements (pronouns, particles, dialect)
5. `translation_compensations` provides strategies for OTHER languages to preserve character voice
6. ALL other descriptions should be in the specified output language ({output_lang})
7. `untranslatable_elements` lists what is LOST in translation for translator awareness
8. **TRIGGERS must be BALANCED: include both positive and negative emotional triggers**
9. **Positive triggers must be GRANULAR: distinguish mild encouragement from love confession from existential affirmation**
10. Trigger descriptions should be MEANING-BASED (an LLM judges activation by semantic understanding)

## IMPORTANT NOTES
- identity_core describes Iâ‚€ (who they ARE); conflict_axes/triggers describe Ln (how they REACT)
- Focus on TRANSLATABLE features (how speech changes with emotion)
- z_mode determines the TYPE of breakdown
- z_leak determines the MARKERS of that breakdown
- Characters who DON'T hesitate should have hesitation: 0
- Characters who use denial should specify negation_type: "concealment" (hide feelings), "counter" (deny other's claim), or "declaration" (assert will)
- age_expression_rules should match the character's mental_maturity
- A character's RECOVERY behavior is just as important as their BREAKDOWN behavior for translation"""


# =============================================================================
# FUNCTIONS
# =============================================================================

def build_user_prompt(name: str, source: str, description: str, 
                      output_lang: str, search_context: str = "") -> str:
    """Build the user prompt for persona generation."""
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, "English")
    
    prompt = f"""Generate a v3.3 persona YAML for:

Name: {name}
Source: {source}
Description: {description}
Output Language: {output_lang} ({lang_name})
"""
    
    if search_context:
        prompt += f"""
## Additional Context (from research):
{search_context}
"""
    
    prompt += f"""
Output ONLY valid YAML. No explanation text before or after the YAML.
Start with "# =====" header comment.

REMEMBER:
- `identity_core.essence` is REQUIRED â€” describe who this character IS, not just how they react
- **USE the research context above** to fill: likes, hobbies, personality, first_person variants
- first_person_variants must include ALL variants (including rare/extreme state ones found in research)
- Other identity_core fields (joys, likes, dislikes, etc.) are optional â€” include what you find
- `original_speech_patterns` MUST be in the character's native/source language
- All other descriptions in {lang_name}
- `translation_compensations` provides strategies for preserving voice across languages
- age_context should ONLY contain background info, NOT expression patterns
- **TRIGGERS: Include at least 2-3 positive triggers (z_drop, z_recovery, z_shock) with different granularity**
- **DO NOT collapse all positive inputs into a single "encouragement" trigger**
- A character's recovery/positive reactions are just as important as their breakdown patterns"""
    
    return prompt


def _research_character(client, name: str, source: str, description: str, 
                        model: str) -> str:
    """Pass 1: Research character details using web search.
    
    Returns a text summary of search findings for use as context in generation.
    """
    
    research_prompt = f"""Research the following character for a persona YAML generation.
Search for their wiki page, speech patterns, personality, likes/hobbies, and relationships.

Character: {name}
Source: {source}
Description: {description}

SEARCH PROTOCOL (execute ALL):
1. Search: "{name} {source} wiki" â€” for background, personality, relationships
2. Search: "{name} ä¸€äººç§°" or "{name} speech patterns" â€” for first-person pronouns (ALL variants including rare/extreme ones), sentence endings, dialect, catchphrases
3. Search: "{name} {source} personality hobbies likes" â€” for identity_core details

IMPORTANT: For first_person_variants, find ALL variants including ones used only in extreme emotional states.
Characters who use third-person self-reference (è‡ªåˆ†ã®åå‰ã§è‡ªå·±è¨€åŠ) may revert to standard 
first-person pronouns (ç§/åƒ•/ä¿º) under emotional extremity â€” always check for this.

After searching, output a structured summary of your findings:

## Background
(what you found about their history, role, relationships)

## Speech Patterns  
(first-person pronoun and ALL variants with contexts, sentence endings, catchphrases, dialect)

## Personality & Identity
(likes, dislikes, hobbies, joys, personality traits, what they're like when relaxed)

## Key Relationships
(important relationships and dynamics)

## Emotional Patterns
(how they react under stress, what triggers them positively/negatively)

Only include information you actually found. Do NOT invent details."""

    print("   ğŸ“– Pass 1: Researching character via web search...")
    
    response = client.messages.create(
        model=model,
        max_tokens=4000,
        tools=[
            {
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 5
            }
        ],
        messages=[
            {"role": "user", "content": research_prompt}
        ]
    )
    
    # Extract text and count searches
    research_text = ""
    search_count = 0
    for block in response.content:
        if block.type == "text":
            research_text = block.text  # Last text block has the summary
        elif block.type == "web_search_tool_use":
            search_count += 1
    
    print(f"   ğŸ” Web searches performed: {search_count}")
    if search_count == 0:
        print(f"   âš ï¸  Model did not use web search in research pass")
    
    return research_text


def generate_persona(name: str, source: str, description: str,
                     output_lang: str = "ja",
                     search_context: str = "", 
                     model: str = DEFAULT_MODEL,
                     thinking_budget: int = 0,
                     no_search: bool = False,
                     no_wait: bool = False) -> str:
    """Generate persona YAML using Claude API with web search.
    
    Two-pass approach:
      Pass 1 (Research): web_search to gather character details (no thinking)
      Pass 2 (Generate): thinking to generate YAML with research context (no search)
    
    This separation avoids thinking + web_search compatibility issues.
    
    Args:
        thinking_budget: If > 0, enable extended thinking with this token budget.
                        Recommended: 10000-16000 for complex characters.
        no_search: If True, disable web search (LLM knowledge only).
    """
    
    client = Anthropic()
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, output_lang)
    print(f"ğŸ¯ Generating persona v3.3 for: {name} ({source})")
    print(f"   Output language: {lang_name}")
    print(f"   Model: {model}")
    if no_search:
        print(f"   ğŸ” Web search: OFF (LLM knowledge only)")
    else:
        print(f"   ğŸ” Web search: ON (two-pass: research â†’ generate)")
    if thinking_budget > 0:
        print(f"   ğŸ§  Thinking mode: ON (budget: {thinking_budget} tokens)")
    print()
    
    # === PASS 1: RESEARCH (web search, no thinking) ===
    research_context = ""
    if not no_search:
        research_context = _research_character(client, name, source, description, model)
        # Rate limit protection: wait between passes
        # Tier 1 Opus: 8K output tokens/min â€” Pass 1 uses ~2-3K, Pass 2 needs the rest
        if not no_wait:
            print("   â³ Waiting 60s for rate limit reset (Tier 1: 8K output tokens/min)...")
            print("   ğŸ’¡ Use --no-wait to skip (if you have Tier 2+ API key)")
            time.sleep(60)
        else:
            print("   âš¡ Skipping rate limit wait (--no-wait)")
    
    # Merge any user-provided context with research results
    combined_context = ""
    if search_context and research_context:
        combined_context = f"## User-provided context:\n{search_context}\n\n## Web research results:\n{research_context}"
    elif research_context:
        combined_context = research_context
    elif search_context:
        combined_context = search_context
    
    # === PASS 2: GENERATE YAML (thinking, no search) ===
    system_prompt = build_system_prompt(output_lang)
    user_prompt = build_user_prompt(name, source, description, output_lang, combined_context)
    
    if not no_search:
        print("   ğŸ“ Pass 2: Generating persona YAML...")
    
    api_kwargs = {
        "model": model,
        "max_tokens": 16000 if thinking_budget > 0 else 8000,
        "system": system_prompt,
        "messages": [
            {"role": "user", "content": user_prompt}
        ]
    }
    
    if thinking_budget > 0:
        api_kwargs["thinking"] = {
            "type": "enabled",
            "budget_tokens": thinking_budget
        }
    
    response = client.messages.create(**api_kwargs)
    
    # Extract YAML from response (skip thinking blocks)
    yaml_content = ""
    for block in response.content:
        if block.type == "text":
            yaml_content = block.text  # Last text block wins
    
    # === ROBUST YAML EXTRACTION ===
    # Model may output: explanation text â†’ code block or raw YAML
    # Strategy: try multiple extraction methods in order of reliability
    yaml_content = _extract_yaml(yaml_content)
    
    return yaml_content.strip()


def _extract_yaml(raw: str) -> str:
    """Extract YAML content from model output, handling various formats.
    
    The model may output:
    1. Pure YAML (ideal)
    2. ```yaml ... ``` code block (common)
    3. Preamble text + ```yaml ... ``` (with thinking mode)
    4. Preamble text + raw YAML without code fences (worst case)
    """
    
    # Method 1: Extract from ```yaml ... ``` code block
    if "```yaml" in raw:
        yaml_part = raw.split("```yaml", 1)[1]
        if "```" in yaml_part:
            yaml_part = yaml_part.split("```", 1)[0]
        return yaml_part.strip()
    
    # Method 2: Extract from generic ``` ... ``` code block
    if "```" in raw:
        parts = raw.split("```")
        # Find the part that looks like YAML (contains "meta:" or starts with "#")
        for part in parts[1::2]:  # odd-indexed parts are inside code fences
            stripped = part.strip()
            if stripped.startswith("# ===") or "meta:" in stripped[:200]:
                return stripped
    
    # Method 3: Find YAML start marker in raw text
    lines = raw.split("\n")
    for i, line in enumerate(lines):
        s = line.strip()
        if s.startswith("# ===") or s.startswith("meta:"):
            return "\n".join(lines[i:])
    
    # Method 4: Find "persona:" or "identity_core:" as fallback start markers
    for i, line in enumerate(lines):
        s = line.strip()
        if s.startswith("persona:") or s.startswith("identity_core:"):
            # Include from this line, but check if meta: is a few lines above
            search_start = max(0, i - 5)
            for j in range(search_start, i):
                if lines[j].strip().startswith("meta:"):
                    return "\n".join(lines[j:])
            return "\n".join(lines[i:])
    
    # Method 5: Last resort â€” return as-is and let validator catch it
    print("   âš ï¸  Could not reliably extract YAML from model output")
    return raw


def validate_v33_persona(yaml_content: str) -> tuple[bool, list[str]]:
    """
    Validate that the generated YAML conforms to v3.3 schema.
    Returns (is_valid, list_of_issues).
    """
    import yaml as yaml_lib
    
    issues = []
    
    try:
        data = yaml_lib.safe_load(yaml_content)
    except yaml_lib.YAMLError as e:
        return False, [f"YAML parse error: {e}"]
    
    # Check meta version
    meta_version = data.get("meta", {}).get("version", "")
    if meta_version not in ["3.0", "3.1", "3.2", "3.3"]:
        issues.append(f"meta.version should be '3.3' (got '{meta_version}')")
    
    # === v3.3 IDENTITY_CORE CHECK ===
    identity_core = data.get("identity_core", {})
    if not identity_core:
        issues.append(
            "v3.3 requires identity_core section â€” describes WHO the character IS (Iâ‚€). "
            "At minimum, identity_core.essence is required."
        )
    elif not identity_core.get("essence"):
        issues.append(
            "identity_core.essence is REQUIRED â€” a 1-2 sentence description of "
            "who this character is, independent of their conflicts."
        )
    # === PROFILE CHECK ===
    persona_info = data.get("persona", {})
    profile = persona_info.get("profile", {})
    if not profile:
        # å¾Œæ–¹äº’æ›: summaryãŒã‚ã‚Œã°warningã ã‘
        if persona_info.get("summary"):
            issues.append(
                "v3.3 prefers persona.profile over persona.summary. "
                "profile should include: background, personality_core, key_relationships, narrative_role"
            )
        else:
            issues.append("persona.profile is required in v3.3")
    elif not profile.get("background"):
        issues.append("persona.profile.background is required")
        
    # Check language structure for v3.1+
    language_data = data.get("language", {})
    
    # Check original_speech_patterns
    osp = language_data.get("original_speech_patterns", {})
    if not osp:
        issues.append("language.original_speech_patterns is required in v3.1+")
    else:
        if "source_lang" not in osp:
            issues.append("original_speech_patterns.source_lang is required")
        if "first_person" not in osp:
            issues.append("original_speech_patterns.first_person is required")
    
    # Check translation_compensations
    tc = language_data.get("translation_compensations", {})
    if not tc:
        issues.append("language.translation_compensations is required in v3.1+")
    
    # Check age structure
    age_data = data.get("age", {})
    if "mental_maturity" not in age_data:
        issues.append("age.mental_maturity is required")
    
    # Check emotion_states for z_mode and z_leak
    emotion_states = data.get("emotion_states", [])
    for i, state in enumerate(emotion_states):
        if "z_mode" not in state:
            issues.append(f"emotion_states[{i}].z_mode is required")
        if "z_leak" not in state:
            issues.append(f"emotion_states[{i}].z_leak is required")
    
    # Check age_expression_rules exists
    if "age_expression_rules" not in data:
        issues.append("age_expression_rules is required")
    
    # === v3.2 TRIGGER BALANCE CHECK ===
    triggers = data.get("triggers", [])
    if not triggers:
        issues.append("triggers section is required")
    else:
        positive_count = 0
        negative_count = 0
        
        for t in triggers:
            z_delta_str = str(t.get("z_delta", "+0.0"))
            try:
                z_delta_val = float(z_delta_str.replace("+", ""))
            except ValueError:
                z_delta_val = 0.0
            
            if z_delta_val < 0:
                positive_count += 1  # negative delta = positive trigger (recovery)
            elif z_delta_val > 0:
                negative_count += 1  # positive delta = negative trigger (stress)
        
        if positive_count < 2:
            issues.append(
                f"v3.2 requires at least 2 positive triggers (z_drop/z_recovery/z_shock), "
                f"found {positive_count}. Positive triggers must be granular â€” "
                f"do not collapse all positive inputs into one 'encouragement' trigger."
            )
        if negative_count < 2:
            issues.append(
                f"v3.2 requires at least 2 negative triggers (z_spike/z_boost), "
                f"found {negative_count}."
            )
    
    return len(issues) == 0, issues


def save_persona(yaml_content: str, name: str, output_lang: str, 
                 output_dir: str = "personas") -> str:
    """Save generated persona to file."""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate safe filename
    safe_name = name.lower().replace(" ", "_").replace("ãƒ»", "_")
    safe_name = "".join(c for c in safe_name if c.isalnum() or c == "_")
    
    # Include language in filename if not Japanese
    if output_lang != "ja":
        filename = f"{safe_name}_v33_{output_lang}.yaml"
    else:
        filename = f"{safe_name}_v33.yaml"
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(yaml_content)
    
    return filepath


def list_languages():
    """Print supported languages."""
    print("Supported output languages:")
    print("-" * 40)
    for code, name in SUPPORTED_LANGUAGES.items():
        print(f"  {code:4} : {name}")
    print()


def main():
    parser = argparse.ArgumentParser(
        description="Generate persona YAML v3.3 for Z-Axis Translation System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Japanese output (default) â€” with web search
  python persona_generator.py --name "ç‰§ç€¬ç´…è‰æ –" --source "Steins;Gate" \\
    --desc "ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¤©æ‰ç§‘å­¦è€…"

  # Without web search (LLM knowledge only)
  python persona_generator.py --name "ç‰§ç€¬ç´…è‰æ –" --source "Steins;Gate" \\
    --desc "ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¤©æ‰ç§‘å­¦è€…" --no-search

  # English output
  python persona_generator.py --name "Kurisu Makise" --source "Steins;Gate" \\
    --desc "Tsundere genius scientist" --lang en

  # Chinese output
  python persona_generator.py --name "ç‰§æ¿‘çº¢è‰æ –" --source "å‘½è¿çŸ³ä¹‹é—¨" \\
    --desc "å‚²å¨‡å¤©æ‰ç§‘å­¦å®¶" --lang zh

  # With validation
  python persona_generator.py --name "ãƒŠãƒ„ã‚­ãƒ»ã‚¹ãƒãƒ«" --source "Re:Zero" \\
    --desc "æ­»ã«æˆ»ã‚Šèƒ½åŠ›è€…" --validate

  # With extended thinking + web search (maximum quality)
  python persona_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate" \\
    --desc "å¤©ç„¶ç™’ã—ç³»ã®å¹¼é¦´æŸ“" --thinking 10000

  # List supported languages
  python persona_generator.py --list-languages
        """
    )
    parser.add_argument("--name", help="Character name")
    parser.add_argument("--source", help="Source work (anime, game, etc.)")
    parser.add_argument("--desc", help="Brief character description")
    parser.add_argument("--lang", default="ja", choices=list(SUPPORTED_LANGUAGES.keys()),
                        help="Output language for descriptions (default: ja)")
    parser.add_argument("--context", default="", help="Additional context or search results")
    parser.add_argument("--context-file", help="File containing additional context")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Model to use")
    parser.add_argument("--thinking", type=int, default=0, metavar="BUDGET",
                        help="Enable extended thinking with token budget (e.g. --thinking 10000)")
    parser.add_argument("--no-search", action="store_true",
                        help="Disable web search (use LLM knowledge only). Default: search enabled")
    parser.add_argument("--no-wait", action="store_true",
                        help="Skip rate limit wait between passes (for Tier 2+ API keys)")
    parser.add_argument("--output-dir", default="personas", help="Output directory")
    parser.add_argument("--print-only", action="store_true", help="Print YAML without saving")
    parser.add_argument("--validate", action="store_true", help="Validate v3.3 schema compliance")
    parser.add_argument("--list-languages", action="store_true", help="List supported output languages")
    
    args = parser.parse_args()
    
    # Handle --list-languages
    if args.list_languages:
        list_languages()
        return
    
    # Check required arguments
    if not args.name or not args.source or not args.desc:
        parser.error("--name, --source, and --desc are required (unless using --list-languages)")
    
    # Load context from file if provided
    context = args.context
    if args.context_file:
        with open(args.context_file, "r", encoding="utf-8") as f:
            context = f.read()
    
    # Generate persona
    yaml_content = generate_persona(
        name=args.name,
        source=args.source,
        description=args.desc,
        output_lang=args.lang,
        search_context=context,
        model=args.model,
        thinking_budget=args.thinking,
        no_search=args.no_search,
        no_wait=args.no_wait
    )
    
    # Always validate in v3.2 (show warnings)
    is_valid, issues = validate_v33_persona(yaml_content)
    if not is_valid:
        print("âš ï¸  v3.3 Schema Validation Issues:")
        for issue in issues:
            print(f"   - {issue}")
        print()
    else:
        print("âœ… v3.3 Schema Validation: PASSED")
        print()
    
    if args.print_only:
        print(yaml_content)
    else:
        filepath = save_persona(yaml_content, args.name, args.lang, args.output_dir)
        print(f"âœ… Persona v3.3 saved to: {filepath}")
        print()
        print("=" * 60)
        print(yaml_content)


if __name__ == "__main__":
    main()
