#!/usr/bin/env python3
"""
Episode Generator v1.0
======================
Generates Episode Memory YAML files for characters, complementing
persona YAML (who they ARE) with episode data (what they EXPERIENCED).

Uses the same two-pass architecture as persona_generator v3.3:
  Pass 1 (Research): web_search to find canonical episodes
  Pass 2 (Generate): thinking to structure episodes into YAML

Architecture:
  persona_generator â†’ ã€Œã“ã®äººã¯èª°ã‹ã€(personality, speech, conflicts)
  episode_generator â†’ ã€Œã“ã®äººã¯ä½•ã‚’çµŒé¨“ã—ãŸã‹ã€(memories, events, arcs)

Together they provide complete character understanding for Z-axis translation.

Usage:
  python episode_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate" \
    --desc "ãƒ©ãƒœãƒ¡ãƒ³No.002ã€å²¡éƒ¨å€«å¤ªéƒã®å¹¼é¦´æŸ“" --thinking 10000

  python episode_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate 0" \
    --desc "ã‚¼ãƒ­ã®ä¸–ç•Œç·šã®ã¾ã‚†ã‚Š" --thinking 10000 --sequel

Options:
  --thinking N    Enable extended thinking (recommended: 10000)
  --no-search     Skip web search (LLM knowledge only)
  --no-wait       Skip rate limit sleep (Tier 2+ API)
  --sequel        Include sequel/spinoff episodes
  --max-episodes N  Maximum episodes to generate (default: 20, max: 30)
  --persona FILE  Path to existing persona YAML for context
"""

import argparse
import json
import os
import sys
import time
import yaml
from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# Configuration
# ============================================================

DEFAULT_MODEL = "claude-opus-4-5-20251101"

SUPPORTED_LANGUAGES = {
    "ja": "Japanese (æ—¥æœ¬èª)",
    "en": "English",
    "zh": "Chinese (ä¸­æ–‡)",
    "ko": "Korean (í•œêµ­ì–´)",
    "fr": "French (FranÃ§ais)",
    "de": "German (Deutsch)",
    "es": "Spanish (EspaÃ±ol)",
}

# ============================================================
# Episode YAML Schema Definition
# ============================================================

EPISODE_SCHEMA = """
# ===================================================================
# Episode Memory YAML v1.0
# Character: {name}
# Source: {source}
# Generated by: episode_generator
# ===================================================================

meta:
  version: "1.0"
  generated_by: "episode_generator"
  character_id: "{character_id}"  # same as persona YAML
  persona_file: "{persona_file}"  # reference to companion persona YAML
  source: "{source}"
  
# Timeline / world-line organization
# For multi-timeline works (Steins;Gate, Re:Zero, etc.), group by timeline
timelines:
  - timeline_id: "main"
    label: "Main timeline / route"
    episodes: []

# Each episode follows this structure:
# - episode_id: "unique_snake_case_id"
#   timeline: "which timeline this occurs in"
#   chronological_order: 1  # integer for sorting within timeline
#   title: "Short descriptive title"
#   source_episode: "Episode 12" or "Chapter 5" or "Route: True End"
#   
#   summary: |
#     What happened in this episode. Focus on the CHARACTER's experience,
#     not just plot events. What did they see, feel, learn, lose?
#   
#   participants:
#     - name: "Other character"
#       role: "ally / antagonist / catalyst / witness"
#   
#   emotional_impact: "low / medium / high / critical"
#   emotional_detail: |
#     How this episode affected the character emotionally.
#     What changed inside them? What did they carry forward?
#   
#   canonical_quotes:
#     - quote: "Actual dialogue from this scene"
#       context: "Who they're speaking to, what's happening"
#       z_intensity: "low / medium / high"
#       z_mode: "stable / concern / grief / nurture / resolve / release"
#   
#   z_relevance: |
#     How this episode affects translation decisions.
#     What must a translator know about this episode to correctly
#     translate dialogue that references or is shaped by it?
#   
#   character_state_change:
#     before: "Character's state before this episode"
#     after: "Character's state after this episode"  
#     permanent: true/false  # Does this change persist?
#   
#   tags: ["origin_story", "loss", "growth", "relationship", "turning_point"]

# Cross-episode arcs that span multiple episodes
arcs:
  - arc_id: "unique_arc_id"
    title: "Arc title"
    episodes: ["ep_id_1", "ep_id_2", "ep_id_3"]
    theme: "What this arc is about"
    character_growth: |
      How the character changes across this arc.
    z_translation_note: |
      What translators need to know about this arc to handle
      dialogue that references or builds on it.
"""


# ============================================================
# Prompt Builders
# ============================================================

def build_system_prompt(output_lang: str) -> str:
    """Build system prompt for episode YAML generation."""
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, output_lang)
    
    return f"""You are an expert character analyst and narrative researcher specializing in 
fictional character episode documentation for the Divergence-Z translation system.

Your task is to generate an Episode Memory YAML file that captures a character's 
key experiences, memories, and narrative events. This file complements the persona YAML 
(which describes WHO a character is) by documenting WHAT they experienced.

## WHY EPISODE MEMORY MATTERS FOR TRANSLATION

When translating character dialogue, the translator needs to know not just HOW 
the character speaks (persona), but WHAT shaped them (episodes). 

Example: When Mayuri says "I know how hard you've been trying, Okarin," 
a translator needs to know:
- She doesn't have DIRECT memory of the time loops (persona knowledge)
- But she has INTUITIVE understanding from fragments (episode knowledge)
- This line's weight comes from Episode context, not just personality

Without episode context, a translator might render this as simple encouragement.
With episode context, they know it carries the weight of hundreds of unseen deaths.

## OUTPUT FORMAT

Generate a valid YAML file following the Episode Memory YAML v1.0 schema.
All descriptions should be in {lang_name}.
Canonical quotes MUST be in the character's original language.

## CRITICAL RULES

1. **CANONICAL ONLY**: Every episode must be from the actual source material.
   Do NOT invent episodes. If unsure, mark with confidence level.
   
2. **CANONICAL QUOTES**: canonical_quotes must be actual dialogue from the work.
   "Sounds like something they'd say" is NOT acceptable.
   If you cannot find the exact quote, describe the scene without fabricating dialogue.

3. **CHARACTER-CENTRIC**: Describe events from the CHARACTER's perspective.
   Not "Okabe time-leaped" but "Mayuri died again without knowing why."

4. **Z-RELEVANCE**: Every episode must include z_relevance explaining
   how this episode affects translation decisions.

5. **EMOTIONAL TRUTH**: Focus on emotional impact, not just plot mechanics.
   What did this episode FEEL like for the character?

{f'''## CHARACTER RESEARCH CONTEXT
The following research was gathered about this character.
USE this context to ensure episode accuracy and canonical quote fidelity.
''' if True else ''}"""


def build_user_prompt(name: str, source: str, description: str, 
                      output_lang: str, search_context: str = "",
                      persona_context: str = "",
                      max_episodes: int = 20,
                      include_sequel: bool = False) -> str:
    """Build user prompt for episode generation."""
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, output_lang)
    
    prompt = f"""Generate an Episode Memory YAML v1.0 for:

Character: {name}
Source: {source}
Description: {description}
Output Language: {lang_name}
Maximum Episodes: {max_episodes}
{"Include sequel/spinoff episodes: Yes" if include_sequel else "Focus on main work only"}
"""
    
    if persona_context:
        prompt += f"""
## Companion Persona YAML (for reference)
The following persona YAML exists for this character. Use it to understand
their personality, conflicts, and speech patterns. Episodes should be
consistent with the persona's conflict_axes and triggers.

{persona_context}
"""
    
    if search_context:
        prompt += f"""
## Research Context (from web search)
USE this research to ensure episode accuracy and canonical quote fidelity.
Prioritize information from this research over your training data.

{search_context}
"""
    
    prompt += f"""
## EPISODE SELECTION CRITERIA

Select the most impactful episodes by these priorities:
1. **Origin episodes**: Events that shaped the character's core identity
2. **Relationship milestones**: Key moments with important characters  
3. **Turning points**: Where the character's worldview or behavior changed
4. **Trauma/Loss**: Events that left lasting emotional marks
5. **Growth moments**: Where the character overcame limitations
6. **Iconic scenes**: Widely recognized, frequently referenced moments

## EPISODE STRUCTURE

For each episode, include:
- episode_id (unique snake_case)
- timeline (for multi-timeline works)
- chronological_order (integer for sorting)
- title (short, descriptive)
- source_episode (episode/chapter/route number)
- summary (character-centric description)
- participants (who else is involved)
- emotional_impact (low/medium/high/critical)
- emotional_detail (how it affected them)
- canonical_quotes (ACTUAL dialogue, with context and z_mode)
- z_relevance (translation implications)
- character_state_change (before/after/permanent)
- tags

## CANONICAL QUOTES: WHY ACCURACY MATTERS

canonical_quotes serve as GROUND TRUTH for translation.
When a translator encounters dialogue that echoes or references these moments,
they need the original wording to maintain consistency.

Without accurate quotes: translator produces "plausible fabrication"
With accurate quotes: translator produces text that IS the character

If you cannot verify exact wording, use this format:
  - quote: "[unverified] Approximate: ã‚ªã‚«ãƒªãƒ³ã€ã‚‚ã†ã„ã„ã‚“ã ã‚ˆ"
    context: "Scene where Mayuri comforts Okabe"

## ARC STRUCTURE

After listing episodes, identify 2-5 cross-episode arcs that span 
multiple episodes. These help translators understand how dialogue
in later scenes carries weight from earlier events.

Output ONLY valid YAML. No explanation text before or after."""
    
    return prompt


# ============================================================
# Two-Pass Architecture (same as persona_generator v3.3)
# ============================================================

def _research_episodes(client, name: str, source: str, description: str,
                       model: str, include_sequel: bool = False) -> str:
    """Pass 1: Research character episodes using web search.
    
    Uses reason-driven prompting to explain WHY search matters,
    which produces better search coverage than command-style prompts.
    """
    
    sequel_instruction = ""
    if include_sequel:
        sequel_instruction = f"\n4. Search: \"{name} sequel spinoff episodes\" â€” for episodes from related works"
    
    research_prompt = f"""You are a research assistant. Your ONLY job is to use web_search to find information.
Do NOT answer from memory. You MUST search first, then report what you found.

Character: {name}
Source: {source}
Description: {description}

WHY SEARCH IS ESSENTIAL FOR EPISODE MEMORY:
Episode Memory files contain canonical_quotes â€” actual dialogue from specific scenes.
These quotes serve as GROUND TRUTH for a translation system.

The difference between searched and unsearched episode data:
- Without search: "Episode 12: Mayuri probably dies" (vague, maybe wrong episode number)
- With search: "Episode 12 'Dogma in Ergosphere': Mayuri is hit by a train at 7:42 PM" (verified)

LLM memory is especially unreliable for:
- Exact episode numbers and titles
- Which specific scene a quote comes from  
- Chronological order of events across timelines
- Exact wording of dialogue (often paraphrased in training data)

These details DIRECTLY impact translation quality. A translator working from
wrong episode context will misread the weight of every line.

EXECUTE THESE SEARCHES NOW:
1. "{name} {source} wiki plot episodes" â€” story events and episode numbers
2. "{name} {source} åè¨€ åã‚·ãƒ¼ãƒ³ ã‚»ãƒªãƒ•" â€” iconic quotes with scene context
3. "{name} {source} character arc timeline" â€” character development and changes{sequel_instruction}

After ALL searches are complete, compile findings:

## Key Episodes / Scenes
(list major events with VERIFIED episode numbers/chapter numbers)

## Iconic Quotes / Dialogue  
(actual lines found in search results, with scene context)
CRITICAL: Only include quotes you found in search results.
Mark uncertain ones as [approximate].

## Character Arc Summary
(how the character changes, with timeline/episode markers)

## Relationship Milestones
(key moments with other characters, episode-specific)

## Timeline / Route Information
(for multi-timeline works: which events in which timeline/route)

Only include information you actually found in search results."""

    print("   ğŸ“– Pass 1: Researching character episodes via web search...")
    
    # Force search: tool_choice + system prompt + user prompt all insist
    # (Same triple-force pattern that solved persona_generator search 0 problem)
    api_kwargs = {
        "model": model,
        "max_tokens": 10000,
        "system": "You are a research assistant. You MUST use the web_search tool for EVERY request. "
                  "NEVER answer from your own knowledge alone. Always search first, then summarize findings. "
                  "Perform at least 2 separate searches before answering.",
        "tools": [
            {
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": 8
            }
        ],
        "messages": [
            {"role": "user", "content": research_prompt}
        ]
    }
    
    # Try to force tool use (may not work with server-side tools)
    try:
        api_kwargs["tool_choice"] = {"type": "any"}
        response = client.messages.create(**api_kwargs)
    except Exception as e:
        # If tool_choice fails with web_search, retry without it
        print(f"   âš ï¸  tool_choice failed ({type(e).__name__}), retrying without...")
        del api_kwargs["tool_choice"]
        response = client.messages.create(**api_kwargs)
    
    # Extract text and count searches
    # Debug: show all block types to diagnose search detection
    research_text = ""
    search_count = 0
    block_types = []
    for block in response.content:
        block_types.append(block.type)
        if block.type == "text":
            research_text = block.text  # Last text block has the summary
        elif block.type in ("web_search_tool_use", "server_tool_use", "tool_use"):
            search_count += 1
    
    print(f"   ğŸ“Š Response blocks: {block_types}")
    print(f"   ğŸ” Web searches performed: {search_count}")
    if search_count == 0:
        print("   âš ï¸  Model did not use web search in research pass")
        # Show first 200 chars of response for debugging
        if research_text:
            print(f"   ğŸ“„ Response preview: {research_text[:200]}...")
    
    return research_text


def generate_episodes(name: str, source: str, description: str,
                      output_lang: str = "ja",
                      model: str = DEFAULT_MODEL,
                      thinking_budget: int = 0,
                      no_search: bool = False,
                      no_wait: bool = False,
                      include_sequel: bool = False,
                      max_episodes: int = 20,
                      persona_path: str = "") -> str:
    """Generate Episode Memory YAML using Claude API.
    
    Two-pass approach (same as persona_generator v3.3):
      Pass 1 (Research): web_search to find canonical episodes (no thinking)
      Pass 2 (Generate): thinking to structure into YAML (no search)
    
    Args:
        name: Character name
        source: Source work (anime, game, novel, etc.)
        description: Brief character description
        output_lang: Output language code
        model: Claude model to use
        thinking_budget: Extended thinking token budget (0 = disabled)
        no_search: Skip web search
        no_wait: Skip rate limit sleep between passes
        include_sequel: Include sequel/spinoff episodes
        max_episodes: Maximum number of episodes to generate
        persona_path: Path to companion persona YAML for context
    """
    
    client = Anthropic(timeout=600.0)  # 10 minutes
    
    lang_name = SUPPORTED_LANGUAGES.get(output_lang, output_lang)
    print(f"ğŸ“– Generating Episode Memory v1.0 for: {name} ({source})")
    print(f"   Output language: {lang_name}")
    print(f"   Model: {model}")
    print(f"   Max episodes: {max_episodes}")
    if include_sequel:
        print(f"   ğŸ“š Including sequel/spinoff episodes")
    if no_search:
        print(f"   ğŸ” Web search: OFF (LLM knowledge only)")
    else:
        print(f"   ğŸ” Web search: ON (two-pass: research â†’ generate)")
    if thinking_budget > 0:
        print(f"   ğŸ§  Thinking mode: ON (budget: {thinking_budget} tokens)")
    if persona_path:
        print(f"   ğŸ“‹ Persona context: {persona_path}")
    print()
    
    # Load persona YAML if provided
    persona_context = ""
    if persona_path:
        try:
            with open(persona_path, 'r', encoding='utf-8') as f:
                persona_context = f.read()
            print(f"   âœ… Loaded persona YAML ({len(persona_context)} chars)")
        except FileNotFoundError:
            print(f"   âš ï¸  Persona file not found: {persona_path}")
        except Exception as e:
            print(f"   âš ï¸  Error loading persona: {e}")
    
    # === PASS 1: RESEARCH (web search, no thinking) ===
    research_context = ""
    if not no_search:
        research_context = _research_episodes(
            client, name, source, description, model, include_sequel
        )
        
        if not no_wait:
            print("   â³ Waiting 60s for rate limit reset (Tier 1: 8K output tokens/min)...")
            print("   ğŸ’¡ Use --no-wait to skip (if you have Tier 2+ API key)")
            time.sleep(60)
    
    # === PASS 2: GENERATE YAML (thinking, no search) ===
    system_prompt = build_system_prompt(output_lang)
    user_prompt = build_user_prompt(
        name, source, description, output_lang,
        search_context=research_context,
        persona_context=persona_context,
        max_episodes=max_episodes,
        include_sequel=include_sequel
    )
    
    if not no_search:
        print("   ğŸ“ Pass 2: Generating Episode Memory YAML...")
    
    api_kwargs = {
        "model": model,
        "max_tokens": 32000 if thinking_budget > 0 else 16000,
        "system": system_prompt,
        "messages": [
            {"role": "user", "content": user_prompt}
        ]
    }
    
    if thinking_budget > 0:
        api_kwargs["thinking"] = {
            "type": "enabled",
            "budget_tokens": thinking_budget
        }
    
    # Use streaming for long operations (required when >10min)
    yaml_content = ""
    
    with client.messages.stream(**api_kwargs) as stream:
        full_response = stream.get_final_message()
    
    # Extract YAML from response (skip thinking blocks)
    for block in full_response.content:
        if block.type == "text":
            yaml_content = block.text  # Last text block wins
    
    # === ROBUST YAML EXTRACTION ===
    yaml_content = _extract_yaml(yaml_content)
    
    return yaml_content.strip()


# ============================================================
# YAML Extraction (same robust 5-method approach)
# ============================================================

def _extract_yaml(raw: str) -> str:
    """Extract YAML content from model output, handling various formats.
    
    5-method fallback (same as persona_generator v3.3):
    1. Extract from ```yaml ... ``` code block
    2. Extract from generic ``` ... ``` (find YAML-like content)
    3. Find # === or meta: start marker
    4. Find episodes: or timelines: as fallback start markers
    5. Return as-is with warning
    """
    
    # Method 1: Extract from ```yaml ... ``` code block
    if "```yaml" in raw:
        yaml_part = raw.split("```yaml", 1)[1]
        if "```" in yaml_part:
            yaml_part = yaml_part.split("```", 1)[0]
        return yaml_part.strip()
    
    # Method 2: Extract from generic ``` ... ``` code block
    if "```" in raw:
        parts = raw.split("```")
        for part in parts[1::2]:  # odd-indexed parts are inside code fences
            stripped = part.strip()
            if stripped.startswith("# ===") or "meta:" in stripped[:200]:
                return stripped
    
    # Method 3: Find YAML start marker in raw text
    lines = raw.split("\n")
    for i, line in enumerate(lines):
        s = line.strip()
        if s.startswith("# ===") or s.startswith("meta:"):
            return "\n".join(lines[i:])
    
    # Method 4: Find episode-specific markers
    for i, line in enumerate(lines):
        s = line.strip()
        if s.startswith("timelines:") or s.startswith("episodes:"):
            search_start = max(0, i - 5)
            for j in range(search_start, i):
                if lines[j].strip().startswith("meta:"):
                    return "\n".join(lines[j:])
            return "\n".join(lines[i:])
    
    # Method 5: Last resort
    print("   âš ï¸  Could not reliably extract YAML from model output")
    return raw


# ============================================================
# Validation
# ============================================================

def validate_episode_yaml(yaml_content: str) -> tuple:
    """Validate Episode Memory YAML structure.
    
    Returns:
        (is_valid: bool, issues: list[str])
    """
    issues = []
    
    # Try to parse YAML
    try:
        data = yaml.safe_load(yaml_content)
    except yaml.YAMLError as e:
        return False, [f"YAML parse error: {e}"]
    
    if not isinstance(data, dict):
        return False, ["Root must be a dict"]
    
    # Check meta
    if "meta" not in data:
        issues.append("Missing 'meta' section")
    else:
        meta = data["meta"]
        if "version" not in meta:
            issues.append("Missing meta.version")
        if "character_id" not in meta:
            issues.append("Missing meta.character_id")
    
    # Check for episodes (can be in timelines or top-level)
    has_episodes = False
    episode_count = 0
    
    if "timelines" in data:
        for tl in data["timelines"]:
            if not isinstance(tl, dict):
                issues.append(f"Timeline entry is not a dict (got {type(tl).__name__})")
                continue
            if "episodes" in tl and tl["episodes"]:
                has_episodes = True
                episode_count += len(tl["episodes"])
                
                for ep in tl["episodes"]:
                    if not isinstance(ep, dict):
                        issues.append(f"Episode entry is not a dict (got {type(ep).__name__})")
                        continue
                    ep_id = ep.get("episode_id", "unknown")
                    
                    # Check required fields
                    if "episode_id" not in ep:
                        issues.append(f"Episode missing episode_id")
                    if "summary" not in ep:
                        issues.append(f"Episode '{ep_id}' missing summary")
                    if "emotional_impact" not in ep:
                        issues.append(f"Episode '{ep_id}' missing emotional_impact")
                    if "z_relevance" not in ep:
                        issues.append(f"Episode '{ep_id}' missing z_relevance")
                    
                    # Check canonical quotes format
                    if "canonical_quotes" in ep:
                        for q in ep["canonical_quotes"]:
                            if not isinstance(q, dict):
                                issues.append(f"Episode '{ep_id}': quote is not a dict")
                                continue
                            if "quote" not in q:
                                issues.append(f"Episode '{ep_id}': quote missing 'quote' field")
    
    if "episodes" in data and data["episodes"]:
        has_episodes = True
        episode_count += len(data["episodes"])
    
    if not has_episodes:
        issues.append("No episodes found (check timelines[].episodes)")
    
    # Check arcs
    if "arcs" in data and data["arcs"]:
        for arc in data["arcs"]:
            if not isinstance(arc, dict):
                issues.append(f"Arc entry is not a dict (got {type(arc).__name__}: {str(arc)[:50]})")
                continue
            if "arc_id" not in arc:
                issues.append("Arc missing arc_id")
            if "episodes" not in arc:
                issues.append(f"Arc '{arc.get('arc_id', 'unknown')}' missing episodes list")
    
    # Summary
    if episode_count > 0:
        print(f"   ğŸ“Š Episodes found: {episode_count}")
    
    is_valid = len(issues) == 0
    return is_valid, issues


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="Episode Generator v1.0 â€” Generate Episode Memory YAML",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic generation with web search and thinking
  python episode_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate" \\
    --desc "ãƒ©ãƒœãƒ¡ãƒ³No.002" --thinking 10000

  # With persona YAML context
  python episode_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate" \\
    --desc "ãƒ©ãƒœãƒ¡ãƒ³No.002" --thinking 10000 \\
    --persona personas/æ¤åã¾ã‚†ã‚Š_v33.yaml

  # Include sequel episodes
  python episode_generator.py --name "æ¤åã¾ã‚†ã‚Š" --source "Steins;Gate" \\
    --desc "ãƒ©ãƒœãƒ¡ãƒ³No.002" --thinking 10000 --sequel

  # Fast mode (no search, Tier 2+)
  python episode_generator.py --name "ç‰§ç€¬ç´…è‰æ –" --source "Steins;Gate" \\
    --desc "å¤©æ‰è„³ç§‘å­¦è€…" --no-search --no-wait
"""
    )
    
    parser.add_argument("--name", required=True, help="Character name")
    parser.add_argument("--source", required=True, help="Source work")
    parser.add_argument("--desc", default="", help="Brief character description")
    parser.add_argument("--lang", default="ja", choices=SUPPORTED_LANGUAGES.keys(),
                        help="Output language (default: ja)")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Claude model")
    parser.add_argument("--thinking", type=int, default=0,
                        help="Extended thinking budget (recommended: 10000)")
    parser.add_argument("--no-search", action="store_true",
                        help="Disable web search")
    parser.add_argument("--no-wait", action="store_true",
                        help="Skip rate limit sleep (Tier 2+)")
    parser.add_argument("--sequel", action="store_true",
                        help="Include sequel/spinoff episodes")
    parser.add_argument("--max-episodes", type=int, default=20,
                        help="Maximum episodes to generate (default: 20, max: 30)")
    parser.add_argument("--persona", default="",
                        help="Path to companion persona YAML")
    parser.add_argument("--output", "-o", default="",
                        help="Output file path (default: {name}_Episode.yaml)")
    
    args = parser.parse_args()
    
    # Generate
    yaml_content = generate_episodes(
        name=args.name,
        source=args.source,
        description=args.desc,
        output_lang=args.lang,
        model=args.model,
        thinking_budget=args.thinking,
        no_search=args.no_search,
        no_wait=args.no_wait,
        include_sequel=args.sequel,
        max_episodes=min(args.max_episodes, 30),
        persona_path=args.persona,
    )
    
    # Validate
    print()
    is_valid, issues = validate_episode_yaml(yaml_content)
    
    if is_valid:
        print("âœ… Episode YAML validation: PASSED")
    else:
        print(f"âš ï¸  Episode YAML Validation Issues:")
        for issue in issues:
            print(f"   - {issue}")
    
    # Determine output path
    if args.output:
        output_path = args.output
    else:
        # Generate default filename
        safe_name = args.name.replace(" ", "_")
        if args.sequel:
            output_path = f"{safe_name}_Episode_full.yaml"
        else:
            output_path = f"{safe_name}_Episode.yaml"
    
    # Write output
    has_parse_error = any("YAML parse error" in issue for issue in issues)
    
    if has_parse_error:
        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ = Contexté™ç•Œã§åˆ‡ã‚ŒãŸå¯èƒ½æ€§å¤§
        broken_path = output_path.replace(".yaml", "_BROKEN.yaml")
        with open(broken_path, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        print(f"\nâŒ YAML parse error detected â€” file may be truncated (context limit?)")
        print(f"   Broken file saved as: {broken_path}")
        print(f"   â†³ Fix manually or re-run with fewer episodes (--max-episodes)")
        return 1
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(yaml_content)
    
    print(f"\nğŸ“ Episode Memory saved to: {output_path}")
    print(f"   File size: {len(yaml_content):,} characters")
    
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
